{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Beacon1</th>\n",
       "      <th>Beacon2</th>\n",
       "      <th>Beacon3</th>\n",
       "      <th>Beacon4</th>\n",
       "      <th>Beacon5</th>\n",
       "      <th>Beacon6</th>\n",
       "      <th>Beacon7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-66</td>\n",
       "      <td>-67</td>\n",
       "      <td>-81</td>\n",
       "      <td>-77</td>\n",
       "      <td>-87</td>\n",
       "      <td>-85</td>\n",
       "      <td>-82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>-71</td>\n",
       "      <td>-80</td>\n",
       "      <td>-73</td>\n",
       "      <td>-64</td>\n",
       "      <td>-61</td>\n",
       "      <td>-73</td>\n",
       "      <td>-82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-72</td>\n",
       "      <td>-68</td>\n",
       "      <td>-79</td>\n",
       "      <td>-78</td>\n",
       "      <td>-78</td>\n",
       "      <td>-75</td>\n",
       "      <td>-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-84</td>\n",
       "      <td>-71</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>-76</td>\n",
       "      <td>-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-74</td>\n",
       "      <td>-87</td>\n",
       "      <td>-74</td>\n",
       "      <td>-67</td>\n",
       "      <td>-65</td>\n",
       "      <td>-70</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   steps    x    y  Beacon1  Beacon2  Beacon3  Beacon4  Beacon5  Beacon6  \\\n",
       "0      1  5.0  6.5      -66      -67      -81      -77      -87      -85   \n",
       "1      2  5.0  6.6      -71      -80      -73      -64      -61      -73   \n",
       "2      3  5.0  6.7      -72      -68      -79      -78      -78      -75   \n",
       "3      4  5.0  6.8      -84      -71      -72      -69      -65      -76   \n",
       "4      5  5.0  6.9      -74      -87      -74      -67      -65      -70   \n",
       "\n",
       "   Beacon7  \n",
       "0      -82  \n",
       "1      -82  \n",
       "2      -67  \n",
       "3      -72  \n",
       "4      -73  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "data = pd.read_csv('beacon.csv',engine='python')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_dist(p1, p2):\n",
    "    x1,y1 = p1\n",
    "    x2,y2 = p2\n",
    "    x1, y1 = np.array(x1), np.array(y1)\n",
    "    x2, y2 = np.array(x2), np.array(y2)\n",
    "    dx = x1 - x2\n",
    "    dy = y1 - y2\n",
    "    dx = dx ** 2\n",
    "    dy = dy ** 2\n",
    "    dists = dx + dy\n",
    "    dists = np.sqrt(dists)\n",
    "    return np.mean(dists), dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beacon1</th>\n",
       "      <th>Beacon2</th>\n",
       "      <th>Beacon3</th>\n",
       "      <th>Beacon4</th>\n",
       "      <th>Beacon5</th>\n",
       "      <th>Beacon6</th>\n",
       "      <th>Beacon7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-66</td>\n",
       "      <td>-67</td>\n",
       "      <td>-81</td>\n",
       "      <td>-77</td>\n",
       "      <td>-87</td>\n",
       "      <td>-85</td>\n",
       "      <td>-82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-71</td>\n",
       "      <td>-80</td>\n",
       "      <td>-73</td>\n",
       "      <td>-64</td>\n",
       "      <td>-61</td>\n",
       "      <td>-73</td>\n",
       "      <td>-82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-72</td>\n",
       "      <td>-68</td>\n",
       "      <td>-79</td>\n",
       "      <td>-78</td>\n",
       "      <td>-78</td>\n",
       "      <td>-75</td>\n",
       "      <td>-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-84</td>\n",
       "      <td>-71</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>-76</td>\n",
       "      <td>-72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-74</td>\n",
       "      <td>-87</td>\n",
       "      <td>-74</td>\n",
       "      <td>-67</td>\n",
       "      <td>-65</td>\n",
       "      <td>-70</td>\n",
       "      <td>-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>-79</td>\n",
       "      <td>-80</td>\n",
       "      <td>-84</td>\n",
       "      <td>-83</td>\n",
       "      <td>-72</td>\n",
       "      <td>-59</td>\n",
       "      <td>-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>-76</td>\n",
       "      <td>-79</td>\n",
       "      <td>-81</td>\n",
       "      <td>-90</td>\n",
       "      <td>-77</td>\n",
       "      <td>-58</td>\n",
       "      <td>-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>-75</td>\n",
       "      <td>-84</td>\n",
       "      <td>-74</td>\n",
       "      <td>-88</td>\n",
       "      <td>-80</td>\n",
       "      <td>-60</td>\n",
       "      <td>-58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>-83</td>\n",
       "      <td>-87</td>\n",
       "      <td>-86</td>\n",
       "      <td>-84</td>\n",
       "      <td>-81</td>\n",
       "      <td>-65</td>\n",
       "      <td>-58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>-72</td>\n",
       "      <td>-87</td>\n",
       "      <td>-76</td>\n",
       "      <td>-94</td>\n",
       "      <td>-81</td>\n",
       "      <td>-56</td>\n",
       "      <td>-66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1916 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Beacon1  Beacon2  Beacon3  Beacon4  Beacon5  Beacon6  Beacon7\n",
       "0         -66      -67      -81      -77      -87      -85      -82\n",
       "1         -71      -80      -73      -64      -61      -73      -82\n",
       "2         -72      -68      -79      -78      -78      -75      -67\n",
       "3         -84      -71      -72      -69      -65      -76      -72\n",
       "4         -74      -87      -74      -67      -65      -70      -73\n",
       "...       ...      ...      ...      ...      ...      ...      ...\n",
       "1911      -79      -80      -84      -83      -72      -59      -62\n",
       "1912      -76      -79      -81      -90      -77      -58      -61\n",
       "1913      -75      -84      -74      -88      -80      -60      -58\n",
       "1914      -83      -87      -86      -84      -81      -65      -58\n",
       "1915      -72      -87      -76      -94      -81      -56      -66\n",
       "\n",
       "[1916 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beacon_values = data.iloc[:,3:]\n",
    "beacon_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1916, 10)\n",
      "[[0.65384615 0.75       0.38297872 ... 0.22       0.29545455 0.32075472]\n",
      " [0.55769231 0.45454545 0.55319149 ... 0.74       0.56818182 0.32075472]\n",
      " [0.53846154 0.72727273 0.42553191 ... 0.4        0.52272727 0.60377358]\n",
      " ...\n",
      " [0.48076923 0.36363636 0.53191489 ... 0.36       0.86363636 0.77358491]\n",
      " [0.32692308 0.29545455 0.27659574 ... 0.34       0.75       0.77358491]\n",
      " [0.53846154 0.29545455 0.4893617  ... 0.34       0.95454545 0.62264151]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "beacon_values = data.iloc[:,3:].values\n",
    "print(data.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.iloc[:,3:]\n",
    "# print(X)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaler = scaler.fit_transform(X)\n",
    "y=data.iloc[:,1:3]\n",
    "print(X_scaler)\n",
    "coordinates=data.iloc[:,1:3].values \n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, coordinates, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1532, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1532, 7)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1532 samples, validate on 384 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 16.7819 - mean_squared_error: 16.7819 - val_loss: 10.8982 - val_mean_squared_error: 10.8982\n",
      "Epoch 2/200\n",
      " - 0s - loss: 9.3083 - mean_squared_error: 9.3083 - val_loss: 9.3515 - val_mean_squared_error: 9.3515\n",
      "Epoch 3/200\n",
      " - 1s - loss: 9.3260 - mean_squared_error: 9.3260 - val_loss: 9.8311 - val_mean_squared_error: 9.8311\n",
      "Epoch 4/200\n",
      " - 1s - loss: 9.0826 - mean_squared_error: 9.0826 - val_loss: 10.1728 - val_mean_squared_error: 10.1728\n",
      "Epoch 5/200\n",
      " - 1s - loss: 8.8822 - mean_squared_error: 8.8822 - val_loss: 10.0058 - val_mean_squared_error: 10.0058\n",
      "Epoch 6/200\n",
      " - 1s - loss: 9.1116 - mean_squared_error: 9.1116 - val_loss: 17.9663 - val_mean_squared_error: 17.9663\n",
      "Epoch 7/200\n",
      " - 1s - loss: 8.9236 - mean_squared_error: 8.9236 - val_loss: 12.5196 - val_mean_squared_error: 12.5196\n",
      "Epoch 8/200\n",
      " - 1s - loss: 8.6787 - mean_squared_error: 8.6787 - val_loss: 10.5885 - val_mean_squared_error: 10.5885\n",
      "Epoch 9/200\n",
      " - 1s - loss: 8.7416 - mean_squared_error: 8.7416 - val_loss: 9.2132 - val_mean_squared_error: 9.2132\n",
      "Epoch 10/200\n",
      " - 1s - loss: 8.8402 - mean_squared_error: 8.8402 - val_loss: 9.0095 - val_mean_squared_error: 9.0095\n",
      "Epoch 11/200\n",
      " - 1s - loss: 8.7423 - mean_squared_error: 8.7423 - val_loss: 9.0201 - val_mean_squared_error: 9.0201\n",
      "Epoch 12/200\n",
      " - 1s - loss: 8.7351 - mean_squared_error: 8.7351 - val_loss: 8.8309 - val_mean_squared_error: 8.8309\n",
      "Epoch 13/200\n",
      " - 1s - loss: 8.7539 - mean_squared_error: 8.7539 - val_loss: 9.0730 - val_mean_squared_error: 9.0730\n",
      "Epoch 14/200\n",
      " - 1s - loss: 8.6767 - mean_squared_error: 8.6767 - val_loss: 9.8274 - val_mean_squared_error: 9.8274\n",
      "Epoch 15/200\n",
      " - 1s - loss: 8.6268 - mean_squared_error: 8.6268 - val_loss: 8.7557 - val_mean_squared_error: 8.7557\n",
      "Epoch 16/200\n",
      " - 1s - loss: 8.6020 - mean_squared_error: 8.6020 - val_loss: 8.9944 - val_mean_squared_error: 8.9944\n",
      "Epoch 17/200\n",
      " - 1s - loss: 8.6283 - mean_squared_error: 8.6283 - val_loss: 10.1616 - val_mean_squared_error: 10.1616\n",
      "Epoch 18/200\n",
      " - 1s - loss: 8.6632 - mean_squared_error: 8.6632 - val_loss: 9.4390 - val_mean_squared_error: 9.4390\n",
      "Epoch 19/200\n",
      " - 1s - loss: 8.3920 - mean_squared_error: 8.3920 - val_loss: 8.7346 - val_mean_squared_error: 8.7346\n",
      "Epoch 20/200\n",
      " - 1s - loss: 8.4935 - mean_squared_error: 8.4935 - val_loss: 8.7149 - val_mean_squared_error: 8.7149\n",
      "Epoch 21/200\n",
      " - 1s - loss: 8.4959 - mean_squared_error: 8.4959 - val_loss: 8.9558 - val_mean_squared_error: 8.9558\n",
      "Epoch 22/200\n",
      " - 1s - loss: 8.5313 - mean_squared_error: 8.5313 - val_loss: 9.9186 - val_mean_squared_error: 9.9186\n",
      "Epoch 23/200\n",
      " - 1s - loss: 8.3780 - mean_squared_error: 8.3780 - val_loss: 8.8721 - val_mean_squared_error: 8.8721\n",
      "Epoch 24/200\n",
      " - 1s - loss: 8.4013 - mean_squared_error: 8.4013 - val_loss: 8.7463 - val_mean_squared_error: 8.7463\n",
      "Epoch 25/200\n",
      " - 1s - loss: 8.5579 - mean_squared_error: 8.5579 - val_loss: 9.1494 - val_mean_squared_error: 9.1494\n",
      "Epoch 26/200\n",
      " - 1s - loss: 8.5126 - mean_squared_error: 8.5126 - val_loss: 8.6590 - val_mean_squared_error: 8.6590\n",
      "Epoch 27/200\n",
      " - 1s - loss: 8.3123 - mean_squared_error: 8.3123 - val_loss: 9.6258 - val_mean_squared_error: 9.6258\n",
      "Epoch 28/200\n",
      " - 1s - loss: 8.4350 - mean_squared_error: 8.4350 - val_loss: 9.5843 - val_mean_squared_error: 9.5843\n",
      "Epoch 29/200\n",
      " - 1s - loss: 8.5381 - mean_squared_error: 8.5381 - val_loss: 8.9724 - val_mean_squared_error: 8.9724\n",
      "Epoch 30/200\n",
      " - 1s - loss: 8.4418 - mean_squared_error: 8.4418 - val_loss: 9.2992 - val_mean_squared_error: 9.2992\n",
      "Epoch 31/200\n",
      " - 1s - loss: 8.4663 - mean_squared_error: 8.4663 - val_loss: 8.6783 - val_mean_squared_error: 8.6783\n",
      "Epoch 32/200\n",
      " - 1s - loss: 8.3432 - mean_squared_error: 8.3432 - val_loss: 9.6722 - val_mean_squared_error: 9.6722\n",
      "Epoch 33/200\n",
      " - 1s - loss: 8.3922 - mean_squared_error: 8.3922 - val_loss: 8.8519 - val_mean_squared_error: 8.8519\n",
      "Epoch 34/200\n",
      " - 1s - loss: 8.5070 - mean_squared_error: 8.5070 - val_loss: 12.3599 - val_mean_squared_error: 12.3599\n",
      "Epoch 35/200\n",
      " - 1s - loss: 8.3116 - mean_squared_error: 8.3116 - val_loss: 9.0066 - val_mean_squared_error: 9.0066\n",
      "Epoch 36/200\n",
      " - 1s - loss: 8.2192 - mean_squared_error: 8.2192 - val_loss: 8.5355 - val_mean_squared_error: 8.5355\n",
      "Epoch 37/200\n",
      " - 1s - loss: 8.3520 - mean_squared_error: 8.3520 - val_loss: 9.6863 - val_mean_squared_error: 9.6863\n",
      "Epoch 38/200\n",
      " - 1s - loss: 8.2364 - mean_squared_error: 8.2364 - val_loss: 8.6379 - val_mean_squared_error: 8.6379\n",
      "Epoch 39/200\n",
      " - 1s - loss: 8.0847 - mean_squared_error: 8.0847 - val_loss: 8.8348 - val_mean_squared_error: 8.8348\n",
      "Epoch 40/200\n",
      " - 1s - loss: 8.2033 - mean_squared_error: 8.2033 - val_loss: 9.2907 - val_mean_squared_error: 9.2907\n",
      "Epoch 41/200\n",
      " - 1s - loss: 8.4110 - mean_squared_error: 8.4110 - val_loss: 8.8180 - val_mean_squared_error: 8.8180\n",
      "Epoch 42/200\n",
      " - 1s - loss: 8.3054 - mean_squared_error: 8.3054 - val_loss: 8.9571 - val_mean_squared_error: 8.9571\n",
      "Epoch 43/200\n",
      " - 1s - loss: 8.1140 - mean_squared_error: 8.1140 - val_loss: 9.1314 - val_mean_squared_error: 9.1314\n",
      "Epoch 44/200\n",
      " - 1s - loss: 8.2962 - mean_squared_error: 8.2962 - val_loss: 9.0177 - val_mean_squared_error: 9.0177\n",
      "Epoch 45/200\n",
      " - 1s - loss: 8.2005 - mean_squared_error: 8.2005 - val_loss: 8.9411 - val_mean_squared_error: 8.9411\n",
      "Epoch 46/200\n",
      " - 1s - loss: 8.2347 - mean_squared_error: 8.2347 - val_loss: 9.8009 - val_mean_squared_error: 9.8009\n",
      "Epoch 47/200\n",
      " - 1s - loss: 8.2365 - mean_squared_error: 8.2365 - val_loss: 10.2113 - val_mean_squared_error: 10.2113\n",
      "Epoch 48/200\n",
      " - 1s - loss: 8.1353 - mean_squared_error: 8.1353 - val_loss: 8.7649 - val_mean_squared_error: 8.7649\n",
      "Epoch 49/200\n",
      " - 1s - loss: 8.2561 - mean_squared_error: 8.2561 - val_loss: 8.8925 - val_mean_squared_error: 8.8925\n",
      "Epoch 50/200\n",
      " - 1s - loss: 8.1587 - mean_squared_error: 8.1587 - val_loss: 9.7902 - val_mean_squared_error: 9.7902\n",
      "Epoch 51/200\n",
      " - 1s - loss: 8.1602 - mean_squared_error: 8.1602 - val_loss: 9.4844 - val_mean_squared_error: 9.4844\n",
      "Epoch 52/200\n",
      " - 1s - loss: 8.0433 - mean_squared_error: 8.0433 - val_loss: 8.7777 - val_mean_squared_error: 8.7777\n",
      "Epoch 53/200\n",
      " - 1s - loss: 8.1884 - mean_squared_error: 8.1884 - val_loss: 9.4153 - val_mean_squared_error: 9.4153\n",
      "Epoch 54/200\n",
      " - 1s - loss: 7.9604 - mean_squared_error: 7.9604 - val_loss: 9.1200 - val_mean_squared_error: 9.1200\n",
      "Epoch 55/200\n",
      " - 1s - loss: 8.1041 - mean_squared_error: 8.1041 - val_loss: 8.8855 - val_mean_squared_error: 8.8855\n",
      "Epoch 56/200\n",
      " - 1s - loss: 8.1173 - mean_squared_error: 8.1173 - val_loss: 9.1710 - val_mean_squared_error: 9.1710\n",
      "Epoch 57/200\n",
      " - 1s - loss: 8.0021 - mean_squared_error: 8.0021 - val_loss: 8.7364 - val_mean_squared_error: 8.7364\n",
      "Epoch 58/200\n",
      " - 1s - loss: 7.9271 - mean_squared_error: 7.9271 - val_loss: 8.8819 - val_mean_squared_error: 8.8819\n",
      "Epoch 59/200\n",
      " - 1s - loss: 8.0003 - mean_squared_error: 8.0003 - val_loss: 8.8924 - val_mean_squared_error: 8.8924\n",
      "Epoch 60/200\n",
      " - 1s - loss: 8.1356 - mean_squared_error: 8.1356 - val_loss: 10.7327 - val_mean_squared_error: 10.7327\n",
      "Epoch 61/200\n",
      " - 1s - loss: 7.9537 - mean_squared_error: 7.9537 - val_loss: 9.8497 - val_mean_squared_error: 9.8497\n",
      "Epoch 62/200\n",
      " - 1s - loss: 7.9769 - mean_squared_error: 7.9769 - val_loss: 9.2618 - val_mean_squared_error: 9.2618\n",
      "Epoch 63/200\n",
      " - 1s - loss: 8.1366 - mean_squared_error: 8.1366 - val_loss: 9.2273 - val_mean_squared_error: 9.2273\n",
      "Epoch 64/200\n",
      " - 1s - loss: 7.9901 - mean_squared_error: 7.9901 - val_loss: 8.7272 - val_mean_squared_error: 8.7272\n",
      "Epoch 65/200\n",
      " - 1s - loss: 8.0041 - mean_squared_error: 8.0041 - val_loss: 8.9227 - val_mean_squared_error: 8.9227\n",
      "Epoch 66/200\n",
      " - 1s - loss: 7.8432 - mean_squared_error: 7.8432 - val_loss: 8.4903 - val_mean_squared_error: 8.4903\n",
      "Epoch 67/200\n",
      " - 1s - loss: 8.1664 - mean_squared_error: 8.1664 - val_loss: 8.9409 - val_mean_squared_error: 8.9409\n",
      "Epoch 68/200\n",
      " - 1s - loss: 8.1825 - mean_squared_error: 8.1825 - val_loss: 8.5386 - val_mean_squared_error: 8.5386\n",
      "Epoch 69/200\n",
      " - 1s - loss: 8.0059 - mean_squared_error: 8.0059 - val_loss: 9.3417 - val_mean_squared_error: 9.3417\n",
      "Epoch 70/200\n",
      " - 1s - loss: 8.1643 - mean_squared_error: 8.1643 - val_loss: 8.9090 - val_mean_squared_error: 8.9090\n",
      "Epoch 71/200\n",
      " - 1s - loss: 8.0677 - mean_squared_error: 8.0677 - val_loss: 10.8082 - val_mean_squared_error: 10.8082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200\n",
      " - 1s - loss: 7.8719 - mean_squared_error: 7.8719 - val_loss: 9.8011 - val_mean_squared_error: 9.8011\n",
      "Epoch 73/200\n",
      " - 1s - loss: 8.0363 - mean_squared_error: 8.0363 - val_loss: 8.7984 - val_mean_squared_error: 8.7984\n",
      "Epoch 74/200\n",
      " - 1s - loss: 7.9893 - mean_squared_error: 7.9893 - val_loss: 8.9935 - val_mean_squared_error: 8.9935\n",
      "Epoch 75/200\n",
      " - 1s - loss: 8.0028 - mean_squared_error: 8.0028 - val_loss: 8.5434 - val_mean_squared_error: 8.5434\n",
      "Epoch 76/200\n",
      " - 1s - loss: 8.0345 - mean_squared_error: 8.0345 - val_loss: 9.1888 - val_mean_squared_error: 9.1888\n",
      "Epoch 77/200\n",
      " - 1s - loss: 7.8360 - mean_squared_error: 7.8360 - val_loss: 9.0442 - val_mean_squared_error: 9.0442\n",
      "Epoch 78/200\n",
      " - 1s - loss: 7.9542 - mean_squared_error: 7.9542 - val_loss: 9.0011 - val_mean_squared_error: 9.0011\n",
      "Epoch 79/200\n",
      " - 1s - loss: 7.9517 - mean_squared_error: 7.9517 - val_loss: 9.3627 - val_mean_squared_error: 9.3627\n",
      "Epoch 80/200\n",
      " - 1s - loss: 8.0256 - mean_squared_error: 8.0256 - val_loss: 9.0417 - val_mean_squared_error: 9.0417\n",
      "Epoch 81/200\n",
      " - 1s - loss: 8.0599 - mean_squared_error: 8.0599 - val_loss: 8.7160 - val_mean_squared_error: 8.7160\n",
      "Epoch 82/200\n",
      " - 1s - loss: 7.8882 - mean_squared_error: 7.8882 - val_loss: 8.9305 - val_mean_squared_error: 8.9305\n",
      "Epoch 83/200\n",
      " - 1s - loss: 7.9365 - mean_squared_error: 7.9365 - val_loss: 9.1574 - val_mean_squared_error: 9.1574\n",
      "Epoch 84/200\n",
      " - 1s - loss: 7.9690 - mean_squared_error: 7.9690 - val_loss: 9.2408 - val_mean_squared_error: 9.2408\n",
      "Epoch 85/200\n",
      " - 1s - loss: 7.9476 - mean_squared_error: 7.9476 - val_loss: 9.3427 - val_mean_squared_error: 9.3427\n",
      "Epoch 86/200\n",
      " - 1s - loss: 8.0158 - mean_squared_error: 8.0158 - val_loss: 8.6428 - val_mean_squared_error: 8.6428\n",
      "Epoch 87/200\n",
      " - 1s - loss: 7.9156 - mean_squared_error: 7.9156 - val_loss: 8.7716 - val_mean_squared_error: 8.7716\n",
      "Epoch 88/200\n",
      " - 1s - loss: 7.9204 - mean_squared_error: 7.9204 - val_loss: 9.2048 - val_mean_squared_error: 9.2048\n",
      "Epoch 89/200\n",
      " - 1s - loss: 7.8824 - mean_squared_error: 7.8824 - val_loss: 8.7889 - val_mean_squared_error: 8.7889\n",
      "Epoch 90/200\n",
      " - 1s - loss: 7.9205 - mean_squared_error: 7.9205 - val_loss: 9.3902 - val_mean_squared_error: 9.3902\n",
      "Epoch 91/200\n",
      " - 1s - loss: 7.9760 - mean_squared_error: 7.9760 - val_loss: 8.9426 - val_mean_squared_error: 8.9426\n",
      "Epoch 92/200\n",
      " - 1s - loss: 7.9998 - mean_squared_error: 7.9998 - val_loss: 12.2806 - val_mean_squared_error: 12.2806\n",
      "Epoch 93/200\n",
      " - 1s - loss: 8.0369 - mean_squared_error: 8.0369 - val_loss: 9.4589 - val_mean_squared_error: 9.4589\n",
      "Epoch 94/200\n",
      " - 1s - loss: 7.8077 - mean_squared_error: 7.8077 - val_loss: 9.2762 - val_mean_squared_error: 9.2762\n",
      "Epoch 95/200\n",
      " - 1s - loss: 7.9277 - mean_squared_error: 7.9277 - val_loss: 10.2912 - val_mean_squared_error: 10.2912\n",
      "Epoch 96/200\n",
      " - 1s - loss: 8.0079 - mean_squared_error: 8.0079 - val_loss: 8.7844 - val_mean_squared_error: 8.7844\n",
      "Epoch 97/200\n",
      " - 1s - loss: 7.9452 - mean_squared_error: 7.9452 - val_loss: 14.3832 - val_mean_squared_error: 14.3832\n",
      "Epoch 98/200\n",
      " - 1s - loss: 7.9450 - mean_squared_error: 7.9450 - val_loss: 9.2211 - val_mean_squared_error: 9.2211\n",
      "Epoch 99/200\n",
      " - 1s - loss: 7.9040 - mean_squared_error: 7.9040 - val_loss: 9.8845 - val_mean_squared_error: 9.8845\n",
      "Epoch 100/200\n",
      " - 1s - loss: 7.8469 - mean_squared_error: 7.8469 - val_loss: 9.3273 - val_mean_squared_error: 9.3273\n",
      "Epoch 101/200\n",
      " - 1s - loss: 7.7564 - mean_squared_error: 7.7564 - val_loss: 8.8241 - val_mean_squared_error: 8.8241\n",
      "Epoch 102/200\n",
      " - 1s - loss: 7.8909 - mean_squared_error: 7.8909 - val_loss: 9.7464 - val_mean_squared_error: 9.7464\n",
      "Epoch 103/200\n",
      " - 1s - loss: 8.0637 - mean_squared_error: 8.0637 - val_loss: 9.7767 - val_mean_squared_error: 9.7767\n",
      "Epoch 104/200\n",
      " - 1s - loss: 7.9124 - mean_squared_error: 7.9124 - val_loss: 9.1670 - val_mean_squared_error: 9.1670\n",
      "Epoch 105/200\n",
      " - 1s - loss: 7.9282 - mean_squared_error: 7.9282 - val_loss: 9.1793 - val_mean_squared_error: 9.1793\n",
      "Epoch 106/200\n",
      " - 1s - loss: 8.0355 - mean_squared_error: 8.0355 - val_loss: 8.9796 - val_mean_squared_error: 8.9796\n",
      "Epoch 107/200\n",
      " - 1s - loss: 8.0690 - mean_squared_error: 8.0690 - val_loss: 8.6570 - val_mean_squared_error: 8.6570\n",
      "Epoch 108/200\n",
      " - 1s - loss: 7.8429 - mean_squared_error: 7.8429 - val_loss: 8.9177 - val_mean_squared_error: 8.9177\n",
      "Epoch 109/200\n",
      " - 0s - loss: 8.1498 - mean_squared_error: 8.1498 - val_loss: 8.4090 - val_mean_squared_error: 8.4090\n",
      "Epoch 110/200\n",
      " - 0s - loss: 7.9514 - mean_squared_error: 7.9514 - val_loss: 8.5800 - val_mean_squared_error: 8.5800\n",
      "Epoch 111/200\n",
      " - 0s - loss: 7.7638 - mean_squared_error: 7.7638 - val_loss: 8.7880 - val_mean_squared_error: 8.7880\n",
      "Epoch 112/200\n",
      " - 0s - loss: 7.8491 - mean_squared_error: 7.8491 - val_loss: 9.1625 - val_mean_squared_error: 9.1625\n",
      "Epoch 113/200\n",
      " - 0s - loss: 7.8730 - mean_squared_error: 7.8730 - val_loss: 8.8655 - val_mean_squared_error: 8.8655\n",
      "Epoch 114/200\n",
      " - 0s - loss: 7.9074 - mean_squared_error: 7.9074 - val_loss: 9.4580 - val_mean_squared_error: 9.4580\n",
      "Epoch 115/200\n",
      " - 0s - loss: 7.7569 - mean_squared_error: 7.7569 - val_loss: 9.6384 - val_mean_squared_error: 9.6384\n",
      "Epoch 116/200\n",
      " - 1s - loss: 7.8343 - mean_squared_error: 7.8343 - val_loss: 8.8208 - val_mean_squared_error: 8.8208\n",
      "Epoch 117/200\n",
      " - 1s - loss: 7.9334 - mean_squared_error: 7.9334 - val_loss: 9.1712 - val_mean_squared_error: 9.1712\n",
      "Epoch 118/200\n",
      " - 1s - loss: 7.9203 - mean_squared_error: 7.9203 - val_loss: 8.6079 - val_mean_squared_error: 8.6079\n",
      "Epoch 119/200\n",
      " - 1s - loss: 7.8996 - mean_squared_error: 7.8996 - val_loss: 9.3250 - val_mean_squared_error: 9.3250\n",
      "Epoch 120/200\n",
      " - 1s - loss: 7.8115 - mean_squared_error: 7.8115 - val_loss: 8.9285 - val_mean_squared_error: 8.9285\n",
      "Epoch 121/200\n",
      " - 1s - loss: 7.7942 - mean_squared_error: 7.7942 - val_loss: 8.7588 - val_mean_squared_error: 8.7588\n",
      "Epoch 122/200\n",
      " - 1s - loss: 7.8096 - mean_squared_error: 7.8096 - val_loss: 8.8226 - val_mean_squared_error: 8.8226\n",
      "Epoch 123/200\n",
      " - 1s - loss: 8.0549 - mean_squared_error: 8.0549 - val_loss: 8.5548 - val_mean_squared_error: 8.5548\n",
      "Epoch 124/200\n",
      " - 1s - loss: 7.7493 - mean_squared_error: 7.7493 - val_loss: 8.6344 - val_mean_squared_error: 8.6344\n",
      "Epoch 125/200\n",
      " - 1s - loss: 7.9685 - mean_squared_error: 7.9685 - val_loss: 9.8547 - val_mean_squared_error: 9.8547\n",
      "Epoch 126/200\n",
      " - 1s - loss: 7.9030 - mean_squared_error: 7.9030 - val_loss: 8.8137 - val_mean_squared_error: 8.8137\n",
      "Epoch 127/200\n",
      " - 1s - loss: 7.9063 - mean_squared_error: 7.9063 - val_loss: 8.9061 - val_mean_squared_error: 8.9061\n",
      "Epoch 128/200\n",
      " - 1s - loss: 7.9621 - mean_squared_error: 7.9621 - val_loss: 8.9003 - val_mean_squared_error: 8.9003\n",
      "Epoch 129/200\n",
      " - 1s - loss: 7.8982 - mean_squared_error: 7.8982 - val_loss: 9.1215 - val_mean_squared_error: 9.1215\n",
      "Epoch 130/200\n",
      " - 1s - loss: 7.8381 - mean_squared_error: 7.8381 - val_loss: 8.9609 - val_mean_squared_error: 8.9609\n",
      "Epoch 131/200\n",
      " - 1s - loss: 7.9280 - mean_squared_error: 7.9280 - val_loss: 9.4625 - val_mean_squared_error: 9.4625\n",
      "Epoch 132/200\n",
      " - 1s - loss: 7.9093 - mean_squared_error: 7.9093 - val_loss: 9.8334 - val_mean_squared_error: 9.8334\n",
      "Epoch 133/200\n",
      " - 1s - loss: 7.8108 - mean_squared_error: 7.8108 - val_loss: 9.7830 - val_mean_squared_error: 9.7830\n",
      "Epoch 134/200\n",
      " - 1s - loss: 7.8218 - mean_squared_error: 7.8218 - val_loss: 9.0986 - val_mean_squared_error: 9.0986\n",
      "Epoch 135/200\n",
      " - 1s - loss: 7.8081 - mean_squared_error: 7.8081 - val_loss: 9.6598 - val_mean_squared_error: 9.6598\n",
      "Epoch 136/200\n",
      " - 1s - loss: 7.8867 - mean_squared_error: 7.8867 - val_loss: 9.2271 - val_mean_squared_error: 9.2271\n",
      "Epoch 137/200\n",
      " - 1s - loss: 7.8641 - mean_squared_error: 7.8641 - val_loss: 9.4180 - val_mean_squared_error: 9.4180\n",
      "Epoch 138/200\n",
      " - 1s - loss: 7.9237 - mean_squared_error: 7.9237 - val_loss: 8.4811 - val_mean_squared_error: 8.4811\n",
      "Epoch 139/200\n",
      " - 1s - loss: 7.9182 - mean_squared_error: 7.9182 - val_loss: 10.1412 - val_mean_squared_error: 10.1412\n",
      "Epoch 140/200\n",
      " - 1s - loss: 7.6809 - mean_squared_error: 7.6809 - val_loss: 9.7611 - val_mean_squared_error: 9.7611\n",
      "Epoch 141/200\n",
      " - 1s - loss: 7.9368 - mean_squared_error: 7.9368 - val_loss: 8.6936 - val_mean_squared_error: 8.6936\n",
      "Epoch 142/200\n",
      " - 1s - loss: 7.7754 - mean_squared_error: 7.7754 - val_loss: 9.0701 - val_mean_squared_error: 9.0701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/200\n",
      " - 1s - loss: 7.8268 - mean_squared_error: 7.8268 - val_loss: 8.5511 - val_mean_squared_error: 8.5511\n",
      "Epoch 144/200\n",
      " - 1s - loss: 7.6415 - mean_squared_error: 7.6415 - val_loss: 10.7637 - val_mean_squared_error: 10.7637\n",
      "Epoch 145/200\n",
      " - 1s - loss: 7.7047 - mean_squared_error: 7.7047 - val_loss: 8.6175 - val_mean_squared_error: 8.6175\n",
      "Epoch 146/200\n",
      " - 1s - loss: 7.8032 - mean_squared_error: 7.8032 - val_loss: 8.9619 - val_mean_squared_error: 8.9619\n",
      "Epoch 147/200\n",
      " - 1s - loss: 7.8927 - mean_squared_error: 7.8927 - val_loss: 8.8328 - val_mean_squared_error: 8.8328\n",
      "Epoch 148/200\n",
      " - 1s - loss: 7.7229 - mean_squared_error: 7.7229 - val_loss: 8.5396 - val_mean_squared_error: 8.5396\n",
      "Epoch 149/200\n",
      " - 1s - loss: 7.8076 - mean_squared_error: 7.8076 - val_loss: 8.7893 - val_mean_squared_error: 8.7893\n",
      "Epoch 150/200\n",
      " - 1s - loss: 7.9418 - mean_squared_error: 7.9418 - val_loss: 9.7792 - val_mean_squared_error: 9.7792\n",
      "Epoch 151/200\n",
      " - 1s - loss: 7.8763 - mean_squared_error: 7.8763 - val_loss: 8.6746 - val_mean_squared_error: 8.6746\n",
      "Epoch 152/200\n",
      " - 1s - loss: 7.7439 - mean_squared_error: 7.7439 - val_loss: 8.6361 - val_mean_squared_error: 8.6361\n",
      "Epoch 153/200\n",
      " - 1s - loss: 7.7997 - mean_squared_error: 7.7997 - val_loss: 8.5924 - val_mean_squared_error: 8.5924\n",
      "Epoch 154/200\n",
      " - 0s - loss: 7.8940 - mean_squared_error: 7.8940 - val_loss: 8.9204 - val_mean_squared_error: 8.9204\n",
      "Epoch 155/200\n",
      " - 0s - loss: 7.8519 - mean_squared_error: 7.8519 - val_loss: 8.6467 - val_mean_squared_error: 8.6467\n",
      "Epoch 156/200\n",
      " - 0s - loss: 7.9782 - mean_squared_error: 7.9782 - val_loss: 8.7530 - val_mean_squared_error: 8.7530\n",
      "Epoch 157/200\n",
      " - 0s - loss: 7.7558 - mean_squared_error: 7.7558 - val_loss: 8.9020 - val_mean_squared_error: 8.9020\n",
      "Epoch 158/200\n",
      " - 1s - loss: 7.7451 - mean_squared_error: 7.7451 - val_loss: 9.4681 - val_mean_squared_error: 9.4681\n",
      "Epoch 159/200\n",
      " - 1s - loss: 7.8326 - mean_squared_error: 7.8326 - val_loss: 8.5859 - val_mean_squared_error: 8.5859\n",
      "Epoch 160/200\n",
      " - 1s - loss: 7.5502 - mean_squared_error: 7.5502 - val_loss: 8.8930 - val_mean_squared_error: 8.8930\n",
      "Epoch 161/200\n",
      " - 1s - loss: 7.6319 - mean_squared_error: 7.6319 - val_loss: 8.7512 - val_mean_squared_error: 8.7512\n",
      "Epoch 162/200\n",
      " - 1s - loss: 7.6839 - mean_squared_error: 7.6839 - val_loss: 9.4085 - val_mean_squared_error: 9.4085\n",
      "Epoch 163/200\n",
      " - 1s - loss: 7.7439 - mean_squared_error: 7.7439 - val_loss: 8.8708 - val_mean_squared_error: 8.8708\n",
      "Epoch 164/200\n",
      " - 1s - loss: 7.8297 - mean_squared_error: 7.8297 - val_loss: 8.7030 - val_mean_squared_error: 8.7030\n",
      "Epoch 165/200\n",
      " - 1s - loss: 7.6272 - mean_squared_error: 7.6272 - val_loss: 9.3079 - val_mean_squared_error: 9.3079\n",
      "Epoch 166/200\n",
      " - 1s - loss: 7.6894 - mean_squared_error: 7.6894 - val_loss: 8.7426 - val_mean_squared_error: 8.7426\n",
      "Epoch 167/200\n",
      " - 1s - loss: 7.8217 - mean_squared_error: 7.8217 - val_loss: 9.0412 - val_mean_squared_error: 9.0412\n",
      "Epoch 168/200\n",
      " - 1s - loss: 7.7694 - mean_squared_error: 7.7694 - val_loss: 8.6934 - val_mean_squared_error: 8.6934\n",
      "Epoch 169/200\n",
      " - 1s - loss: 7.6321 - mean_squared_error: 7.6321 - val_loss: 9.0619 - val_mean_squared_error: 9.0619\n",
      "Epoch 170/200\n",
      " - 1s - loss: 7.8424 - mean_squared_error: 7.8424 - val_loss: 9.1192 - val_mean_squared_error: 9.1192\n",
      "Epoch 171/200\n",
      " - 1s - loss: 7.5812 - mean_squared_error: 7.5812 - val_loss: 9.5117 - val_mean_squared_error: 9.5117\n",
      "Epoch 172/200\n",
      " - 1s - loss: 7.6995 - mean_squared_error: 7.6995 - val_loss: 8.8734 - val_mean_squared_error: 8.8734\n",
      "Epoch 173/200\n",
      " - 1s - loss: 7.6310 - mean_squared_error: 7.6310 - val_loss: 8.7697 - val_mean_squared_error: 8.7697\n",
      "Epoch 174/200\n",
      " - 1s - loss: 7.6585 - mean_squared_error: 7.6585 - val_loss: 8.6168 - val_mean_squared_error: 8.6168\n",
      "Epoch 175/200\n",
      " - 0s - loss: 7.7262 - mean_squared_error: 7.7262 - val_loss: 8.7510 - val_mean_squared_error: 8.7510\n",
      "Epoch 176/200\n",
      " - 1s - loss: 7.7099 - mean_squared_error: 7.7099 - val_loss: 9.2767 - val_mean_squared_error: 9.2767\n",
      "Epoch 177/200\n",
      " - 1s - loss: 7.7215 - mean_squared_error: 7.7215 - val_loss: 9.1075 - val_mean_squared_error: 9.1075\n",
      "Epoch 178/200\n",
      " - 1s - loss: 7.6782 - mean_squared_error: 7.6782 - val_loss: 9.5489 - val_mean_squared_error: 9.5489\n",
      "Epoch 179/200\n",
      " - 1s - loss: 7.7200 - mean_squared_error: 7.7200 - val_loss: 8.8445 - val_mean_squared_error: 8.8445\n",
      "Epoch 180/200\n",
      " - 1s - loss: 7.4945 - mean_squared_error: 7.4945 - val_loss: 8.6760 - val_mean_squared_error: 8.6760\n",
      "Epoch 181/200\n",
      " - 1s - loss: 7.5531 - mean_squared_error: 7.5531 - val_loss: 8.5126 - val_mean_squared_error: 8.5126\n",
      "Epoch 182/200\n",
      " - 1s - loss: 7.6609 - mean_squared_error: 7.6609 - val_loss: 9.2583 - val_mean_squared_error: 9.2583\n",
      "Epoch 183/200\n",
      " - 1s - loss: 7.7613 - mean_squared_error: 7.7613 - val_loss: 9.0468 - val_mean_squared_error: 9.0468\n",
      "Epoch 184/200\n",
      " - 1s - loss: 7.6488 - mean_squared_error: 7.6488 - val_loss: 9.0949 - val_mean_squared_error: 9.0949\n",
      "Epoch 185/200\n",
      " - 1s - loss: 7.7350 - mean_squared_error: 7.7350 - val_loss: 9.7465 - val_mean_squared_error: 9.7465\n",
      "Epoch 186/200\n",
      " - 1s - loss: 7.6583 - mean_squared_error: 7.6583 - val_loss: 8.8491 - val_mean_squared_error: 8.8491\n",
      "Epoch 187/200\n",
      " - 1s - loss: 7.5064 - mean_squared_error: 7.5064 - val_loss: 9.6865 - val_mean_squared_error: 9.6865\n",
      "Epoch 188/200\n",
      " - 1s - loss: 7.6351 - mean_squared_error: 7.6351 - val_loss: 9.3202 - val_mean_squared_error: 9.3202\n",
      "Epoch 189/200\n",
      " - 1s - loss: 7.7494 - mean_squared_error: 7.7494 - val_loss: 9.1542 - val_mean_squared_error: 9.1542\n",
      "Epoch 190/200\n",
      " - 1s - loss: 7.6206 - mean_squared_error: 7.6206 - val_loss: 8.5247 - val_mean_squared_error: 8.5247\n",
      "Epoch 191/200\n",
      " - 1s - loss: 7.5388 - mean_squared_error: 7.5388 - val_loss: 8.6675 - val_mean_squared_error: 8.6675\n",
      "Epoch 192/200\n",
      " - 1s - loss: 7.6330 - mean_squared_error: 7.6330 - val_loss: 8.8704 - val_mean_squared_error: 8.8704\n",
      "Epoch 193/200\n",
      " - 1s - loss: 7.6358 - mean_squared_error: 7.6358 - val_loss: 9.4258 - val_mean_squared_error: 9.4258\n",
      "Epoch 194/200\n",
      " - 1s - loss: 7.7690 - mean_squared_error: 7.7690 - val_loss: 9.7256 - val_mean_squared_error: 9.7256\n",
      "Epoch 195/200\n",
      " - 1s - loss: 7.6701 - mean_squared_error: 7.6701 - val_loss: 9.6035 - val_mean_squared_error: 9.6035\n",
      "Epoch 196/200\n",
      " - 1s - loss: 7.8127 - mean_squared_error: 7.8127 - val_loss: 9.1206 - val_mean_squared_error: 9.1206\n",
      "Epoch 197/200\n",
      " - 1s - loss: 7.7008 - mean_squared_error: 7.7008 - val_loss: 8.4854 - val_mean_squared_error: 8.4854\n",
      "Epoch 198/200\n",
      " - 1s - loss: 7.5755 - mean_squared_error: 7.5755 - val_loss: 8.5422 - val_mean_squared_error: 8.5422\n",
      "Epoch 199/200\n",
      " - 1s - loss: 7.6105 - mean_squared_error: 7.6105 - val_loss: 8.7930 - val_mean_squared_error: 8.7930\n",
      "Epoch 200/200\n",
      " - 1s - loss: 7.6392 - mean_squared_error: 7.6392 - val_loss: 9.9181 - val_mean_squared_error: 9.9181\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "#def create_deep(inp_dim):\n",
    "    #seed = 7\n",
    "    #np.random.seed(seed)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=7,activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "    # Compile model\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mse'])\n",
    "#eturn model\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=600, verbose=2, mode='auto', restore_best_weights=True)\n",
    "#model = create_deep(X_train.shape[1])\n",
    "hist = model.fit(x = X_train, y = Y_train, validation_data = (X_test,Y_test), epochs=200, batch_size=12,  verbose=2, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.664459422478851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = model.predict(X_test)\n",
    "l2dists_mean, l2dists = l2_dist((preds[:, 0], preds[:, 1]), (Y_test[:, 0] , Y_test[:, 0]))\n",
    "print(l2dists_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poulr\\Anaconda32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index:  [ 192  193  194 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 21.3645 - val_loss: 23.2702\n",
      "Epoch 2/200\n",
      " - 0s - loss: 8.0879 - val_loss: 14.0647\n",
      "Epoch 3/200\n",
      " - 0s - loss: 7.4244 - val_loss: 13.4710\n",
      "Epoch 4/200\n",
      " - 0s - loss: 7.2529 - val_loss: 12.6676\n",
      "Epoch 5/200\n",
      " - 0s - loss: 7.2061 - val_loss: 12.0662\n",
      "Epoch 6/200\n",
      " - 0s - loss: 7.1319 - val_loss: 12.0981\n",
      "Epoch 7/200\n",
      " - 0s - loss: 7.0274 - val_loss: 12.2068\n",
      "Epoch 8/200\n",
      " - 0s - loss: 7.1421 - val_loss: 12.5057\n",
      "Epoch 9/200\n",
      " - 0s - loss: 6.9524 - val_loss: 12.0964\n",
      "Epoch 10/200\n",
      " - 0s - loss: 6.9897 - val_loss: 11.4520\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.1110 - val_loss: 11.8721\n",
      "Epoch 12/200\n",
      " - 0s - loss: 7.0263 - val_loss: 11.3616\n",
      "Epoch 13/200\n",
      " - 0s - loss: 6.8457 - val_loss: 11.4450\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.2907 - val_loss: 11.8429\n",
      "Epoch 15/200\n",
      " - 0s - loss: 6.8382 - val_loss: 11.7756\n",
      "Epoch 16/200\n",
      " - 0s - loss: 6.8730 - val_loss: 11.5738\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.0451 - val_loss: 11.5943\n",
      "Epoch 18/200\n",
      " - 0s - loss: 6.9201 - val_loss: 11.5486\n",
      "Epoch 19/200\n",
      " - 0s - loss: 6.7747 - val_loss: 11.3551\n",
      "Epoch 20/200\n",
      " - 0s - loss: 6.7851 - val_loss: 11.7371\n",
      "Epoch 21/200\n",
      " - 0s - loss: 6.7914 - val_loss: 11.2759\n",
      "Epoch 22/200\n",
      " - 0s - loss: 6.7853 - val_loss: 11.4627\n",
      "Epoch 23/200\n",
      " - 0s - loss: 6.7960 - val_loss: 11.8286\n",
      "Epoch 24/200\n",
      " - 0s - loss: 6.7086 - val_loss: 12.0760\n",
      "Epoch 25/200\n",
      " - 0s - loss: 6.7451 - val_loss: 11.2202\n",
      "Epoch 26/200\n",
      " - 0s - loss: 6.7465 - val_loss: 11.2126\n",
      "Epoch 27/200\n",
      " - 0s - loss: 6.7685 - val_loss: 11.5326\n",
      "Epoch 28/200\n",
      " - 0s - loss: 6.6406 - val_loss: 11.7009\n",
      "Epoch 29/200\n",
      " - 0s - loss: 6.6071 - val_loss: 11.3732\n",
      "Epoch 30/200\n",
      " - 0s - loss: 6.5268 - val_loss: 11.7455\n",
      "Epoch 31/200\n",
      " - 0s - loss: 6.8022 - val_loss: 11.2654\n",
      "Epoch 32/200\n",
      " - 0s - loss: 6.7104 - val_loss: 10.9433\n",
      "Epoch 33/200\n",
      " - 0s - loss: 6.5874 - val_loss: 11.0341\n",
      "Epoch 34/200\n",
      " - 0s - loss: 6.6674 - val_loss: 11.5076\n",
      "Epoch 35/200\n",
      " - 0s - loss: 6.6656 - val_loss: 11.7414\n",
      "Epoch 36/200\n",
      " - 0s - loss: 6.6580 - val_loss: 11.4442\n",
      "Epoch 37/200\n",
      " - 0s - loss: 6.5827 - val_loss: 11.2748\n",
      "Epoch 38/200\n",
      " - 0s - loss: 6.6111 - val_loss: 11.5076\n",
      "Epoch 39/200\n",
      " - 0s - loss: 6.6041 - val_loss: 11.6561\n",
      "Epoch 40/200\n",
      " - 0s - loss: 6.4804 - val_loss: 11.5554\n",
      "Epoch 41/200\n",
      " - 0s - loss: 6.5441 - val_loss: 11.3781\n",
      "Epoch 42/200\n",
      " - 0s - loss: 6.5199 - val_loss: 11.9220\n",
      "Epoch 43/200\n",
      " - 0s - loss: 6.4918 - val_loss: 11.6474\n",
      "Epoch 44/200\n",
      " - 0s - loss: 6.6331 - val_loss: 11.6829\n",
      "Epoch 45/200\n",
      " - 0s - loss: 6.5072 - val_loss: 11.2913\n",
      "Epoch 46/200\n",
      " - 0s - loss: 6.4449 - val_loss: 11.2826\n",
      "Epoch 47/200\n",
      " - 0s - loss: 6.4854 - val_loss: 12.0477\n",
      "Epoch 48/200\n",
      " - 0s - loss: 6.7288 - val_loss: 11.5611\n",
      "Epoch 49/200\n",
      " - 0s - loss: 6.5820 - val_loss: 12.3235\n",
      "Epoch 50/200\n",
      " - 0s - loss: 6.4878 - val_loss: 11.3257\n",
      "Epoch 51/200\n",
      " - 0s - loss: 6.5493 - val_loss: 11.6606\n",
      "Epoch 52/200\n",
      " - 0s - loss: 6.4803 - val_loss: 11.3330\n",
      "Epoch 53/200\n",
      " - 0s - loss: 6.4314 - val_loss: 11.4520\n",
      "Epoch 54/200\n",
      " - 0s - loss: 6.5389 - val_loss: 12.4010\n",
      "Epoch 55/200\n",
      " - 0s - loss: 6.4992 - val_loss: 11.3777\n",
      "Epoch 56/200\n",
      " - 0s - loss: 6.3874 - val_loss: 11.2145\n",
      "Epoch 57/200\n",
      " - 0s - loss: 6.4105 - val_loss: 11.2355\n",
      "Epoch 58/200\n",
      " - 0s - loss: 6.3833 - val_loss: 11.2186\n",
      "Epoch 59/200\n",
      " - 0s - loss: 6.4162 - val_loss: 12.0776\n",
      "Epoch 60/200\n",
      " - 0s - loss: 6.4622 - val_loss: 11.0471\n",
      "Epoch 61/200\n",
      " - 0s - loss: 6.3182 - val_loss: 11.6185\n",
      "Epoch 62/200\n",
      " - 0s - loss: 6.6179 - val_loss: 11.6694\n",
      "Epoch 63/200\n",
      " - 0s - loss: 6.3419 - val_loss: 10.8217\n",
      "Epoch 64/200\n",
      " - 0s - loss: 6.2899 - val_loss: 11.3941\n",
      "Epoch 65/200\n",
      " - 0s - loss: 6.3132 - val_loss: 11.9382\n",
      "Epoch 66/200\n",
      " - 0s - loss: 6.2920 - val_loss: 11.9920\n",
      "Epoch 67/200\n",
      " - 0s - loss: 6.3564 - val_loss: 11.4497\n",
      "Epoch 68/200\n",
      " - 0s - loss: 6.4013 - val_loss: 11.1076\n",
      "Epoch 69/200\n",
      " - 0s - loss: 6.3320 - val_loss: 11.2402\n",
      "Epoch 70/200\n",
      " - 0s - loss: 6.2213 - val_loss: 11.3735\n",
      "Epoch 71/200\n",
      " - 0s - loss: 6.2515 - val_loss: 11.6410\n",
      "Epoch 72/200\n",
      " - 0s - loss: 6.2694 - val_loss: 11.3050\n",
      "Epoch 73/200\n",
      " - 0s - loss: 6.3440 - val_loss: 12.3135\n",
      "Epoch 74/200\n",
      " - 0s - loss: 6.1697 - val_loss: 11.1805\n",
      "Epoch 75/200\n",
      " - 0s - loss: 6.4225 - val_loss: 11.6750\n",
      "Epoch 76/200\n",
      " - 0s - loss: 6.2624 - val_loss: 11.0887\n",
      "Epoch 77/200\n",
      " - 0s - loss: 6.2599 - val_loss: 11.3764\n",
      "Epoch 78/200\n",
      " - 0s - loss: 6.1111 - val_loss: 11.7370\n",
      "Epoch 79/200\n",
      " - 0s - loss: 6.2244 - val_loss: 11.7670\n",
      "Epoch 80/200\n",
      " - 0s - loss: 6.1631 - val_loss: 11.4198\n",
      "Epoch 81/200\n",
      " - 0s - loss: 6.3121 - val_loss: 10.8780\n",
      "Epoch 82/200\n",
      " - 0s - loss: 6.2730 - val_loss: 11.3923\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.3298 - val_loss: 11.8099\n",
      "Epoch 84/200\n",
      " - 0s - loss: 6.2307 - val_loss: 12.2273\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.3519 - val_loss: 11.8641\n",
      "Epoch 86/200\n",
      " - 0s - loss: 6.3707 - val_loss: 10.8823\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.2218 - val_loss: 11.1074\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.4059 - val_loss: 11.4968\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.2896 - val_loss: 11.3878\n",
      "Epoch 90/200\n",
      " - 0s - loss: 6.1676 - val_loss: 12.2363\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.2323 - val_loss: 11.3604\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.2272 - val_loss: 12.2327\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.2601 - val_loss: 10.8631\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.2531 - val_loss: 11.0505\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.0928 - val_loss: 10.5633\n",
      "Epoch 96/200\n",
      " - 0s - loss: 6.2066 - val_loss: 12.0665\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.2588 - val_loss: 11.0692\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.1885 - val_loss: 10.8330\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.2408 - val_loss: 10.6924\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.1990 - val_loss: 11.1426\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.2778 - val_loss: 12.1204\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.1765 - val_loss: 11.3021\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.1521 - val_loss: 11.1896\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.2858 - val_loss: 10.9458\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.2234 - val_loss: 11.3351\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.0184 - val_loss: 11.3337\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.0640 - val_loss: 10.7931\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.0389 - val_loss: 11.3519\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.1385 - val_loss: 10.9489\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.0379 - val_loss: 11.3170\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.1304 - val_loss: 11.3574\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.1146 - val_loss: 11.1007\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.0010 - val_loss: 10.8716\n",
      "Epoch 114/200\n",
      " - 0s - loss: 5.9969 - val_loss: 10.9562\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.1011 - val_loss: 10.7012\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.0004 - val_loss: 10.6775\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.0945 - val_loss: 10.5643\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.1146 - val_loss: 11.8918\n",
      "Epoch 119/200\n",
      " - 0s - loss: 5.8811 - val_loss: 11.2283\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.0042 - val_loss: 11.1393\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.0648 - val_loss: 11.2563\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.1192 - val_loss: 11.2484\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.0262 - val_loss: 11.7806\n",
      "Epoch 124/200\n",
      " - 0s - loss: 5.8083 - val_loss: 11.4314\n",
      "Epoch 125/200\n",
      " - 0s - loss: 5.8945 - val_loss: 11.0771\n",
      "Epoch 126/200\n",
      " - 0s - loss: 5.9013 - val_loss: 11.5441\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.0469 - val_loss: 11.7632\n",
      "Epoch 128/200\n",
      " - 0s - loss: 5.8448 - val_loss: 10.9225\n",
      "Epoch 129/200\n",
      " - 0s - loss: 5.9428 - val_loss: 11.7707\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.2227 - val_loss: 12.0194\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.0203 - val_loss: 10.6649\n",
      "Epoch 132/200\n",
      " - 0s - loss: 5.8949 - val_loss: 11.0637\n",
      "Epoch 133/200\n",
      " - 0s - loss: 5.7961 - val_loss: 11.1285\n",
      "Epoch 134/200\n",
      " - 0s - loss: 5.8149 - val_loss: 10.8520\n",
      "Epoch 135/200\n",
      " - 0s - loss: 5.8687 - val_loss: 10.6813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200\n",
      " - 0s - loss: 5.9539 - val_loss: 11.0388\n",
      "Epoch 137/200\n",
      " - 0s - loss: 5.9647 - val_loss: 11.2024\n",
      "Epoch 138/200\n",
      " - 0s - loss: 5.7754 - val_loss: 11.6837\n",
      "Epoch 139/200\n",
      " - 0s - loss: 5.8456 - val_loss: 11.6735\n",
      "Epoch 140/200\n",
      " - 0s - loss: 5.8104 - val_loss: 11.4264\n",
      "Epoch 141/200\n",
      " - 0s - loss: 5.9368 - val_loss: 11.7198\n",
      "Epoch 142/200\n",
      " - 0s - loss: 5.6775 - val_loss: 11.4886\n",
      "Epoch 143/200\n",
      " - 0s - loss: 5.9705 - val_loss: 11.4156\n",
      "Epoch 144/200\n",
      " - 0s - loss: 5.8095 - val_loss: 11.1857\n",
      "Epoch 145/200\n",
      " - 0s - loss: 5.7354 - val_loss: 12.1490\n",
      "Epoch 146/200\n",
      " - 0s - loss: 5.7872 - val_loss: 10.8067\n",
      "Epoch 147/200\n",
      " - 0s - loss: 5.9082 - val_loss: 10.9504\n",
      "Epoch 148/200\n",
      " - 0s - loss: 5.7184 - val_loss: 11.8662\n",
      "Epoch 149/200\n",
      " - 0s - loss: 5.7665 - val_loss: 11.1745\n",
      "Epoch 150/200\n",
      " - 0s - loss: 5.8142 - val_loss: 11.0390\n",
      "Epoch 151/200\n",
      " - 0s - loss: 5.7384 - val_loss: 11.0915\n",
      "Epoch 152/200\n",
      " - 0s - loss: 5.7303 - val_loss: 12.2434\n",
      "Epoch 153/200\n",
      " - 0s - loss: 5.8360 - val_loss: 11.2803\n",
      "Epoch 154/200\n",
      " - 0s - loss: 5.7350 - val_loss: 11.5503\n",
      "Epoch 155/200\n",
      " - 0s - loss: 5.7136 - val_loss: 11.0024\n",
      "Epoch 156/200\n",
      " - 0s - loss: 5.9401 - val_loss: 11.3728\n",
      "Epoch 157/200\n",
      " - 0s - loss: 5.7518 - val_loss: 11.0374\n",
      "Epoch 158/200\n",
      " - 0s - loss: 5.8052 - val_loss: 11.4574\n",
      "Epoch 159/200\n",
      " - 0s - loss: 5.7003 - val_loss: 11.1707\n",
      "Epoch 160/200\n",
      " - 0s - loss: 5.7447 - val_loss: 11.8942\n",
      "Epoch 161/200\n",
      " - 0s - loss: 5.8684 - val_loss: 11.1236\n",
      "Epoch 162/200\n",
      " - 0s - loss: 5.5960 - val_loss: 11.5754\n",
      "Epoch 163/200\n",
      " - 0s - loss: 5.8395 - val_loss: 11.3869\n",
      "Epoch 164/200\n",
      " - 0s - loss: 5.6316 - val_loss: 10.8118\n",
      "Epoch 165/200\n",
      " - 0s - loss: 5.7268 - val_loss: 11.5544\n",
      "Epoch 166/200\n",
      " - 0s - loss: 5.5827 - val_loss: 11.4083\n",
      "Epoch 167/200\n",
      " - 0s - loss: 5.6860 - val_loss: 11.0431\n",
      "Epoch 168/200\n",
      " - 0s - loss: 5.8699 - val_loss: 11.5123\n",
      "Epoch 169/200\n",
      " - 0s - loss: 5.7437 - val_loss: 11.3142\n",
      "Epoch 170/200\n",
      " - 0s - loss: 5.7629 - val_loss: 11.1992\n",
      "Epoch 171/200\n",
      " - 0s - loss: 5.5784 - val_loss: 10.9525\n",
      "Epoch 172/200\n",
      " - 0s - loss: 5.6366 - val_loss: 11.6877\n",
      "Epoch 173/200\n",
      " - 0s - loss: 5.8052 - val_loss: 11.5765\n",
      "Epoch 174/200\n",
      " - 0s - loss: 5.7876 - val_loss: 11.9313\n",
      "Epoch 175/200\n",
      " - 0s - loss: 5.6846 - val_loss: 11.3032\n",
      "Epoch 176/200\n",
      " - 0s - loss: 5.7283 - val_loss: 11.5198\n",
      "Epoch 177/200\n",
      " - 0s - loss: 5.6898 - val_loss: 11.4296\n",
      "Epoch 178/200\n",
      " - 0s - loss: 5.6790 - val_loss: 11.3969\n",
      "Epoch 179/200\n",
      " - 0s - loss: 5.7122 - val_loss: 11.1371\n",
      "Epoch 180/200\n",
      " - 0s - loss: 5.5357 - val_loss: 11.8931\n",
      "Epoch 181/200\n",
      " - 0s - loss: 5.6952 - val_loss: 11.1096\n",
      "Epoch 182/200\n",
      " - 0s - loss: 5.5721 - val_loss: 11.1424\n",
      "Epoch 183/200\n",
      " - 0s - loss: 5.6441 - val_loss: 10.9074\n",
      "Epoch 184/200\n",
      " - 0s - loss: 5.5402 - val_loss: 12.0823\n",
      "Epoch 185/200\n",
      " - 0s - loss: 5.6296 - val_loss: 12.0027\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.1778 - val_loss: 11.3246\n",
      "Epoch 187/200\n",
      " - 0s - loss: 5.7544 - val_loss: 11.5293\n",
      "Epoch 188/200\n",
      " - 0s - loss: 5.6168 - val_loss: 11.1663\n",
      "Epoch 189/200\n",
      " - 0s - loss: 5.5863 - val_loss: 11.8180\n",
      "Epoch 190/200\n",
      " - 0s - loss: 5.7104 - val_loss: 12.0282\n",
      "Epoch 191/200\n",
      " - 0s - loss: 5.7386 - val_loss: 11.1676\n",
      "Epoch 192/200\n",
      " - 0s - loss: 5.6374 - val_loss: 11.6838\n",
      "Epoch 193/200\n",
      " - 0s - loss: 5.6139 - val_loss: 12.1511\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.0239 - val_loss: 11.8521\n",
      "Epoch 195/200\n",
      " - 0s - loss: 5.6020 - val_loss: 11.2661\n",
      "Epoch 196/200\n",
      " - 0s - loss: 5.6724 - val_loss: 11.1200\n",
      "Epoch 197/200\n",
      " - 0s - loss: 5.6872 - val_loss: 11.4802\n",
      "Epoch 198/200\n",
      " - 0s - loss: 5.6256 - val_loss: 11.8554\n",
      "Epoch 199/200\n",
      " - 0s - loss: 5.7299 - val_loss: 11.6221\n",
      "Epoch 200/200\n",
      " - 0s - loss: 5.7535 - val_loss: 11.5949\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209\n",
      " 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353\n",
      " 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371\n",
      " 372 373 374 375 376 377 378 379 380 381 382 383]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 25.5469 - val_loss: 19.0840\n",
      "Epoch 2/200\n",
      " - 0s - loss: 10.5738 - val_loss: 13.2525\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.9211 - val_loss: 12.9431\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.7358 - val_loss: 14.0107\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.6882 - val_loss: 11.3975\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.5657 - val_loss: 12.1698\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.4897 - val_loss: 12.0361\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.3981 - val_loss: 11.7252\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.4304 - val_loss: 12.0149\n",
      "Epoch 10/200\n",
      " - 0s - loss: 8.3244 - val_loss: 12.2638\n",
      "Epoch 11/200\n",
      " - 0s - loss: 8.1231 - val_loss: 12.5743\n",
      "Epoch 12/200\n",
      " - 0s - loss: 8.3795 - val_loss: 12.0419\n",
      "Epoch 13/200\n",
      " - 0s - loss: 8.2202 - val_loss: 13.1368\n",
      "Epoch 14/200\n",
      " - 0s - loss: 8.4602 - val_loss: 12.6618\n",
      "Epoch 15/200\n",
      " - 0s - loss: 8.1065 - val_loss: 12.1190\n",
      "Epoch 16/200\n",
      " - 0s - loss: 8.0137 - val_loss: 13.0368\n",
      "Epoch 17/200\n",
      " - 0s - loss: 8.1301 - val_loss: 12.0778\n",
      "Epoch 18/200\n",
      " - 0s - loss: 8.1320 - val_loss: 12.2324\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.9814 - val_loss: 12.5146\n",
      "Epoch 20/200\n",
      " - 0s - loss: 8.0430 - val_loss: 12.1693\n",
      "Epoch 21/200\n",
      " - 0s - loss: 8.0973 - val_loss: 11.9759\n",
      "Epoch 22/200\n",
      " - 0s - loss: 8.0069 - val_loss: 13.3108\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.9189 - val_loss: 12.7887\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.9185 - val_loss: 11.8682\n",
      "Epoch 25/200\n",
      " - 0s - loss: 8.1203 - val_loss: 13.7268\n",
      "Epoch 26/200\n",
      " - 0s - loss: 8.0334 - val_loss: 14.0164\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.9426 - val_loss: 11.5317\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.8733 - val_loss: 11.4182\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.8652 - val_loss: 12.1626\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.8790 - val_loss: 11.8727\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.8643 - val_loss: 11.7237\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.8913 - val_loss: 11.7678\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.7533 - val_loss: 11.8121\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.7888 - val_loss: 12.6484\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.9280 - val_loss: 11.7716\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.6944 - val_loss: 12.2959\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.7871 - val_loss: 12.3114\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.6452 - val_loss: 12.6483\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.6582 - val_loss: 12.0317\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.7132 - val_loss: 12.5698\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.7612 - val_loss: 11.5306\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.7181 - val_loss: 11.9691\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.6066 - val_loss: 11.8264\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.6465 - val_loss: 12.0710\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.6101 - val_loss: 12.9193\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.7057 - val_loss: 12.1710\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.6840 - val_loss: 11.5522\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.6410 - val_loss: 11.9725\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.5358 - val_loss: 12.2853\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.5185 - val_loss: 11.6699\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.6075 - val_loss: 11.7694\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.5327 - val_loss: 12.1342\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.5803 - val_loss: 11.6646\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.6125 - val_loss: 12.4456\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.5033 - val_loss: 12.8082\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.4977 - val_loss: 12.4371\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.4347 - val_loss: 12.6115\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.5069 - val_loss: 12.4176\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.3654 - val_loss: 12.5913\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.3980 - val_loss: 12.2155\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.3289 - val_loss: 12.4511\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.5282 - val_loss: 11.9113\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.2796 - val_loss: 11.5512\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.4063 - val_loss: 11.8462\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.3420 - val_loss: 12.1373\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.4952 - val_loss: 11.8047\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.5269 - val_loss: 12.6504\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.4302 - val_loss: 11.7435\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.3935 - val_loss: 12.9679\n",
      "Epoch 70/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 7.2883 - val_loss: 11.8207\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.3672 - val_loss: 12.4181\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.3457 - val_loss: 11.9436\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.2491 - val_loss: 12.3016\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.2761 - val_loss: 11.6447\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.3322 - val_loss: 11.8884\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.2129 - val_loss: 11.8171\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.2526 - val_loss: 12.7383\n",
      "Epoch 78/200\n",
      " - 0s - loss: 7.2156 - val_loss: 13.1361\n",
      "Epoch 79/200\n",
      " - 0s - loss: 7.1758 - val_loss: 12.8382\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.0784 - val_loss: 11.8492\n",
      "Epoch 81/200\n",
      " - 0s - loss: 7.0689 - val_loss: 12.5580\n",
      "Epoch 82/200\n",
      " - 0s - loss: 7.1653 - val_loss: 11.9832\n",
      "Epoch 83/200\n",
      " - 0s - loss: 7.0256 - val_loss: 12.5068\n",
      "Epoch 84/200\n",
      " - 0s - loss: 7.0837 - val_loss: 11.9287\n",
      "Epoch 85/200\n",
      " - 0s - loss: 7.1828 - val_loss: 11.8411\n",
      "Epoch 86/200\n",
      " - 0s - loss: 7.1661 - val_loss: 11.2156\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.9509 - val_loss: 12.0885\n",
      "Epoch 88/200\n",
      " - 0s - loss: 7.0263 - val_loss: 12.1641\n",
      "Epoch 89/200\n",
      " - 0s - loss: 7.0679 - val_loss: 12.0322\n",
      "Epoch 90/200\n",
      " - 0s - loss: 7.2147 - val_loss: 12.6425\n",
      "Epoch 91/200\n",
      " - 0s - loss: 7.0740 - val_loss: 12.2994\n",
      "Epoch 92/200\n",
      " - 0s - loss: 7.2294 - val_loss: 12.8805\n",
      "Epoch 93/200\n",
      " - 0s - loss: 7.2147 - val_loss: 13.0210\n",
      "Epoch 94/200\n",
      " - 0s - loss: 7.3681 - val_loss: 12.3918\n",
      "Epoch 95/200\n",
      " - 0s - loss: 7.3393 - val_loss: 12.6615\n",
      "Epoch 96/200\n",
      " - 0s - loss: 7.2447 - val_loss: 12.6685\n",
      "Epoch 97/200\n",
      " - 0s - loss: 7.1082 - val_loss: 11.5987\n",
      "Epoch 98/200\n",
      " - 0s - loss: 7.2232 - val_loss: 12.9172\n",
      "Epoch 99/200\n",
      " - 0s - loss: 7.1546 - val_loss: 12.4588\n",
      "Epoch 100/200\n",
      " - 0s - loss: 7.0488 - val_loss: 12.6092\n",
      "Epoch 101/200\n",
      " - 0s - loss: 7.4014 - val_loss: 13.0284\n",
      "Epoch 102/200\n",
      " - 0s - loss: 7.0320 - val_loss: 11.9565\n",
      "Epoch 103/200\n",
      " - 0s - loss: 7.0047 - val_loss: 12.1952\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.7812 - val_loss: 11.9945\n",
      "Epoch 105/200\n",
      " - 0s - loss: 7.2350 - val_loss: 12.0805\n",
      "Epoch 106/200\n",
      " - 0s - loss: 7.2943 - val_loss: 12.5614\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.8901 - val_loss: 13.7760\n",
      "Epoch 108/200\n",
      " - 0s - loss: 7.1623 - val_loss: 12.4870\n",
      "Epoch 109/200\n",
      " - 0s - loss: 7.0147 - val_loss: 14.4993\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.9290 - val_loss: 15.4355\n",
      "Epoch 111/200\n",
      " - 0s - loss: 7.0885 - val_loss: 14.0578\n",
      "Epoch 112/200\n",
      " - 0s - loss: 7.0211 - val_loss: 13.0819\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.8471 - val_loss: 13.8644\n",
      "Epoch 114/200\n",
      " - 0s - loss: 7.2495 - val_loss: 12.2524\n",
      "Epoch 115/200\n",
      " - 0s - loss: 7.0825 - val_loss: 13.9519\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.8653 - val_loss: 11.8733\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.9120 - val_loss: 12.2250\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.9235 - val_loss: 12.9172\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.9869 - val_loss: 12.1612\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.9918 - val_loss: 12.4026\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.8118 - val_loss: 12.1372\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.9126 - val_loss: 12.2930\n",
      "Epoch 123/200\n",
      " - 0s - loss: 7.4646 - val_loss: 12.1710\n",
      "Epoch 124/200\n",
      " - 0s - loss: 7.1708 - val_loss: 12.2569\n",
      "Epoch 125/200\n",
      " - 0s - loss: 7.0184 - val_loss: 12.5137\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.9295 - val_loss: 12.4279\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.8921 - val_loss: 12.5215\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.9342 - val_loss: 12.8852\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.6588 - val_loss: 12.3257\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.7899 - val_loss: 12.4996\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.9205 - val_loss: 12.3522\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.8385 - val_loss: 12.5807\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.7280 - val_loss: 12.7857\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.9205 - val_loss: 12.2079\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.7265 - val_loss: 12.6563\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.7031 - val_loss: 12.3970\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.7964 - val_loss: 11.7859\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.8809 - val_loss: 12.2463\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.6051 - val_loss: 12.7562\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.6667 - val_loss: 12.3050\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.7301 - val_loss: 12.2585\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.7684 - val_loss: 13.0523\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.7794 - val_loss: 12.3312\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.6799 - val_loss: 12.9411\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.7570 - val_loss: 12.0671\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.7180 - val_loss: 11.7577\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.6511 - val_loss: 12.8202\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.7527 - val_loss: 12.0791\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.7082 - val_loss: 12.2698\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.7054 - val_loss: 12.0827\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.4738 - val_loss: 12.5201\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.5394 - val_loss: 12.9898\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.8489 - val_loss: 13.5471\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.6084 - val_loss: 12.8368\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.7216 - val_loss: 12.4770\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.5906 - val_loss: 12.1919\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.7782 - val_loss: 13.1646\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.7583 - val_loss: 12.0130\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.5558 - val_loss: 12.7813\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.5774 - val_loss: 12.5887\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.6051 - val_loss: 12.2990\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.6086 - val_loss: 12.3921\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.7076 - val_loss: 13.0830\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.5077 - val_loss: 12.6025\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.9102 - val_loss: 12.9083\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.6172 - val_loss: 12.6088\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.4434 - val_loss: 12.3449\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.5059 - val_loss: 11.8032\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.5418 - val_loss: 11.7846\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.7938 - val_loss: 12.4902\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.7469 - val_loss: 13.0815\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.6119 - val_loss: 11.3938\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.5559 - val_loss: 12.5171\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.8586 - val_loss: 12.5034\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.7268 - val_loss: 12.5204\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.6726 - val_loss: 14.3973\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.5589 - val_loss: 13.0893\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.2645 - val_loss: 12.6940\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.7183 - val_loss: 12.0450\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.5929 - val_loss: 12.7436\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.5814 - val_loss: 13.4197\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.4879 - val_loss: 11.8444\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.5012 - val_loss: 12.1417\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.6718 - val_loss: 12.2592\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.5135 - val_loss: 12.2751\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.4659 - val_loss: 12.4535\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.4715 - val_loss: 12.0261\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.5130 - val_loss: 13.0172\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.5708 - val_loss: 12.6430\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.2705 - val_loss: 12.2558\n",
      "Epoch 191/200\n",
      " - 0s - loss: 6.5665 - val_loss: 12.6065\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.4515 - val_loss: 12.1191\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.3576 - val_loss: 12.8509\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.5800 - val_loss: 12.3963\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.3472 - val_loss: 12.6753\n",
      "Epoch 196/200\n",
      " - 0s - loss: 6.4916 - val_loss: 12.5653\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.4254 - val_loss: 12.8777\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.3109 - val_loss: 12.3555\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.3038 - val_loss: 12.7431\n",
      "Epoch 200/200\n",
      " - 0s - loss: 6.2943 - val_loss: 12.4427\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491\n",
      " 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509\n",
      " 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527\n",
      " 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545\n",
      " 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563\n",
      " 564 565 566 567 568 569 570 571 572 573 574 575]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 23.9198 - val_loss: 16.2139\n",
      "Epoch 2/200\n",
      " - 0s - loss: 9.5932 - val_loss: 13.4160\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.4906 - val_loss: 12.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200\n",
      " - 0s - loss: 8.3437 - val_loss: 13.1634\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.3408 - val_loss: 12.2525\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.1998 - val_loss: 12.1935\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.3151 - val_loss: 12.4798\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.1905 - val_loss: 12.1406\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.0939 - val_loss: 11.8974\n",
      "Epoch 10/200\n",
      " - 0s - loss: 7.9138 - val_loss: 11.8512\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.9565 - val_loss: 12.0527\n",
      "Epoch 12/200\n",
      " - 0s - loss: 8.1781 - val_loss: 11.8218\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.9856 - val_loss: 12.7535\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.8717 - val_loss: 11.5425\n",
      "Epoch 15/200\n",
      " - 0s - loss: 7.8465 - val_loss: 11.8831\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.7753 - val_loss: 11.6538\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.7859 - val_loss: 11.9517\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.7728 - val_loss: 12.8683\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.8593 - val_loss: 11.2449\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.8975 - val_loss: 12.8979\n",
      "Epoch 21/200\n",
      " - 0s - loss: 7.5978 - val_loss: 11.5891\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.9529 - val_loss: 12.1029\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.6447 - val_loss: 12.1294\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.7433 - val_loss: 12.6239\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.8396 - val_loss: 11.9373\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.6481 - val_loss: 11.6737\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.7104 - val_loss: 11.8357\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.6155 - val_loss: 12.1002\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.6378 - val_loss: 12.3854\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.4680 - val_loss: 12.1882\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.6776 - val_loss: 12.0855\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.4637 - val_loss: 11.8688\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.6235 - val_loss: 12.2940\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.5182 - val_loss: 11.9474\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.5432 - val_loss: 12.1112\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.4764 - val_loss: 12.2363\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.4297 - val_loss: 12.1988\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.4972 - val_loss: 11.9554\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.4812 - val_loss: 11.8467\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.3359 - val_loss: 12.6008\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.4379 - val_loss: 12.8220\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.5388 - val_loss: 12.5679\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.5223 - val_loss: 12.1174\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.5113 - val_loss: 12.6755\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.3981 - val_loss: 11.8515\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.3908 - val_loss: 11.2927\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.4018 - val_loss: 11.8252\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.4094 - val_loss: 11.4729\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.4950 - val_loss: 11.3104\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.1956 - val_loss: 11.5209\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.4172 - val_loss: 11.9630\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.3117 - val_loss: 12.5115\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.3618 - val_loss: 11.5800\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.4506 - val_loss: 12.5270\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.3711 - val_loss: 11.3799\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.3947 - val_loss: 11.7276\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.2356 - val_loss: 12.2590\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.2633 - val_loss: 12.2726\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.2712 - val_loss: 11.8450\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.3499 - val_loss: 11.9457\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.2744 - val_loss: 12.1213\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.1897 - val_loss: 12.0382\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.1583 - val_loss: 11.4213\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.2617 - val_loss: 12.1463\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.3073 - val_loss: 11.3481\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.1759 - val_loss: 11.7308\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.1189 - val_loss: 12.2388\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.2047 - val_loss: 12.9051\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.1306 - val_loss: 12.6394\n",
      "Epoch 70/200\n",
      " - 0s - loss: 7.2852 - val_loss: 11.5178\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.2395 - val_loss: 11.8772\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.1943 - val_loss: 12.2334\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.1324 - val_loss: 12.4277\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.1177 - val_loss: 11.5780\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.1138 - val_loss: 12.2756\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.1713 - val_loss: 11.9746\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.2451 - val_loss: 11.9750\n",
      "Epoch 78/200\n",
      " - 0s - loss: 7.1184 - val_loss: 11.9611\n",
      "Epoch 79/200\n",
      " - 0s - loss: 7.1545 - val_loss: 11.5603\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.1864 - val_loss: 12.5153\n",
      "Epoch 81/200\n",
      " - 0s - loss: 7.1599 - val_loss: 12.0442\n",
      "Epoch 82/200\n",
      " - 0s - loss: 7.1152 - val_loss: 12.5741\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.9119 - val_loss: 12.6764\n",
      "Epoch 84/200\n",
      " - 0s - loss: 6.9985 - val_loss: 12.4409\n",
      "Epoch 85/200\n",
      " - 0s - loss: 7.0148 - val_loss: 12.5713\n",
      "Epoch 86/200\n",
      " - 0s - loss: 7.0916 - val_loss: 11.8337\n",
      "Epoch 87/200\n",
      " - 0s - loss: 7.0234 - val_loss: 12.5134\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.9900 - val_loss: 11.9415\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.8930 - val_loss: 11.4170\n",
      "Epoch 90/200\n",
      " - 0s - loss: 7.0558 - val_loss: 12.0151\n",
      "Epoch 91/200\n",
      " - 0s - loss: 7.0755 - val_loss: 11.9176\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.8984 - val_loss: 11.7331\n",
      "Epoch 93/200\n",
      " - 0s - loss: 7.2039 - val_loss: 12.6701\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.9964 - val_loss: 12.4254\n",
      "Epoch 95/200\n",
      " - 0s - loss: 7.0173 - val_loss: 12.6951\n",
      "Epoch 96/200\n",
      " - 0s - loss: 6.9754 - val_loss: 11.7313\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.9947 - val_loss: 11.7467\n",
      "Epoch 98/200\n",
      " - 0s - loss: 7.3968 - val_loss: 12.0676\n",
      "Epoch 99/200\n",
      " - 0s - loss: 7.1595 - val_loss: 11.8334\n",
      "Epoch 100/200\n",
      " - 0s - loss: 7.0993 - val_loss: 12.5697\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.9069 - val_loss: 12.2994\n",
      "Epoch 102/200\n",
      " - 0s - loss: 7.0360 - val_loss: 12.7114\n",
      "Epoch 103/200\n",
      " - 0s - loss: 7.0364 - val_loss: 12.0778\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.9004 - val_loss: 11.6985\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.7068 - val_loss: 11.4692\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.9740 - val_loss: 12.0503\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.9710 - val_loss: 12.1369\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.9615 - val_loss: 12.0007\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.8505 - val_loss: 12.3899\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.9485 - val_loss: 12.4384\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.9930 - val_loss: 11.1961\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.9260 - val_loss: 11.2702\n",
      "Epoch 113/200\n",
      " - 0s - loss: 7.1547 - val_loss: 12.3908\n",
      "Epoch 114/200\n",
      " - 0s - loss: 7.2178 - val_loss: 12.2849\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.8704 - val_loss: 11.5870\n",
      "Epoch 116/200\n",
      " - 0s - loss: 7.0820 - val_loss: 11.6595\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.7698 - val_loss: 11.7298\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.7636 - val_loss: 12.0309\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.9529 - val_loss: 12.1631\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.8084 - val_loss: 11.6483\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.6814 - val_loss: 11.6666\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.8532 - val_loss: 12.1592\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.7999 - val_loss: 11.4590\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.7021 - val_loss: 12.4161\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.9646 - val_loss: 11.9304\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.8678 - val_loss: 11.9296\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.7652 - val_loss: 11.9155\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.7719 - val_loss: 11.5430\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.7764 - val_loss: 11.3541\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.7457 - val_loss: 12.6946\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.9799 - val_loss: 11.6144\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.7441 - val_loss: 11.6102\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.5987 - val_loss: 12.0652\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.6514 - val_loss: 11.8348\n",
      "Epoch 135/200\n",
      " - 0s - loss: 7.1617 - val_loss: 12.9124\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.9874 - val_loss: 12.7500\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.8917 - val_loss: 11.6540\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.6783 - val_loss: 11.9696\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.7895 - val_loss: 12.8965\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.6120 - val_loss: 11.5438\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.5799 - val_loss: 11.9533\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.7520 - val_loss: 12.3321\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.8815 - val_loss: 11.2896\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.9585 - val_loss: 12.1135\n",
      "Epoch 145/200\n",
      " - 0s - loss: 7.4426 - val_loss: 12.0511\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.8624 - val_loss: 12.2244\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.7363 - val_loss: 12.2041\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.6879 - val_loss: 12.6644\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.7558 - val_loss: 11.9377\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.5739 - val_loss: 12.0782\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.7105 - val_loss: 11.9416\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.8178 - val_loss: 12.2021\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.6658 - val_loss: 11.7535\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.7043 - val_loss: 11.8807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      " - 0s - loss: 6.4115 - val_loss: 12.2979\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.4958 - val_loss: 12.2616\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.7549 - val_loss: 12.0149\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.7252 - val_loss: 12.6728\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.7046 - val_loss: 11.9130\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.4983 - val_loss: 11.8104\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.4849 - val_loss: 12.0688\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.5044 - val_loss: 11.9113\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.4951 - val_loss: 11.8994\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.9558 - val_loss: 21.4468\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.7697 - val_loss: 11.6854\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.6768 - val_loss: 11.8808\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.5491 - val_loss: 11.7218\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.5014 - val_loss: 11.7970\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.4987 - val_loss: 12.3244\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.5222 - val_loss: 11.2930\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.5982 - val_loss: 12.0786\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.6104 - val_loss: 12.0690\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.4752 - val_loss: 13.2117\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.5921 - val_loss: 12.0022\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.5455 - val_loss: 11.6896\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.6991 - val_loss: 12.2041\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.4633 - val_loss: 11.8592\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.4443 - val_loss: 12.1440\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.4161 - val_loss: 12.5496\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.3814 - val_loss: 12.0382\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.7552 - val_loss: 12.7496\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.5012 - val_loss: 12.5539\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.6393 - val_loss: 12.2799\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.3433 - val_loss: 12.8207\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.3326 - val_loss: 12.2828\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.3130 - val_loss: 12.0798\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.4816 - val_loss: 12.5199\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.3380 - val_loss: 11.9946\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.2618 - val_loss: 12.1738\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.2974 - val_loss: 11.5013\n",
      "Epoch 191/200\n",
      " - 0s - loss: 6.5539 - val_loss: 12.2931\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.6165 - val_loss: 12.6521\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.4512 - val_loss: 12.1071\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.3880 - val_loss: 12.5417\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.6081 - val_loss: 12.4668\n",
      "Epoch 196/200\n",
      " - 0s - loss: 6.3675 - val_loss: 11.9820\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.2869 - val_loss: 12.5054\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.3358 - val_loss: 11.9039\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.2784 - val_loss: 11.6800\n",
      "Epoch 200/200\n",
      " - 0s - loss: 6.3917 - val_loss: 11.8067\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 22.9343 - val_loss: 18.4780\n",
      "Epoch 2/200\n",
      " - 0s - loss: 8.5952 - val_loss: 15.3477\n",
      "Epoch 3/200\n",
      " - 0s - loss: 7.7691 - val_loss: 14.7447\n",
      "Epoch 4/200\n",
      " - 0s - loss: 7.5370 - val_loss: 13.7389\n",
      "Epoch 5/200\n",
      " - 0s - loss: 7.2940 - val_loss: 14.1807\n",
      "Epoch 6/200\n",
      " - 0s - loss: 7.3239 - val_loss: 12.5718\n",
      "Epoch 7/200\n",
      " - 0s - loss: 7.3571 - val_loss: 12.7705\n",
      "Epoch 8/200\n",
      " - 0s - loss: 7.1824 - val_loss: 12.7889\n",
      "Epoch 9/200\n",
      " - 0s - loss: 7.3338 - val_loss: 13.0010\n",
      "Epoch 10/200\n",
      " - 0s - loss: 7.3245 - val_loss: 13.0563\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.0708 - val_loss: 12.7503\n",
      "Epoch 12/200\n",
      " - 0s - loss: 7.1574 - val_loss: 13.2866\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.2038 - val_loss: 13.2899\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.2247 - val_loss: 12.2175\n",
      "Epoch 15/200\n",
      " - 0s - loss: 7.0780 - val_loss: 13.1306\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.0873 - val_loss: 13.6102\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.0011 - val_loss: 12.6567\n",
      "Epoch 18/200\n",
      " - 0s - loss: 6.8411 - val_loss: 12.7386\n",
      "Epoch 19/200\n",
      " - 0s - loss: 6.9985 - val_loss: 13.8771\n",
      "Epoch 20/200\n",
      " - 0s - loss: 6.9600 - val_loss: 13.2399\n",
      "Epoch 21/200\n",
      " - 0s - loss: 6.7727 - val_loss: 12.9428\n",
      "Epoch 22/200\n",
      " - 0s - loss: 6.8340 - val_loss: 12.6860\n",
      "Epoch 23/200\n",
      " - 0s - loss: 6.8105 - val_loss: 12.1821\n",
      "Epoch 24/200\n",
      " - 0s - loss: 6.8603 - val_loss: 12.9803\n",
      "Epoch 25/200\n",
      " - 0s - loss: 6.9478 - val_loss: 13.5540\n",
      "Epoch 26/200\n",
      " - 0s - loss: 6.7873 - val_loss: 12.6227\n",
      "Epoch 27/200\n",
      " - 0s - loss: 6.7724 - val_loss: 11.6590\n",
      "Epoch 28/200\n",
      " - 0s - loss: 6.7282 - val_loss: 12.6167\n",
      "Epoch 29/200\n",
      " - 0s - loss: 6.8006 - val_loss: 12.2959\n",
      "Epoch 30/200\n",
      " - 0s - loss: 6.7463 - val_loss: 11.9725\n",
      "Epoch 31/200\n",
      " - 0s - loss: 6.6534 - val_loss: 12.8927\n",
      "Epoch 32/200\n",
      " - 0s - loss: 6.7177 - val_loss: 12.5727\n",
      "Epoch 33/200\n",
      " - 0s - loss: 6.7379 - val_loss: 12.3701\n",
      "Epoch 34/200\n",
      " - 0s - loss: 6.6517 - val_loss: 11.9409\n",
      "Epoch 35/200\n",
      " - 0s - loss: 6.8422 - val_loss: 12.5209\n",
      "Epoch 36/200\n",
      " - 0s - loss: 6.7777 - val_loss: 13.4058\n",
      "Epoch 37/200\n",
      " - 0s - loss: 6.7489 - val_loss: 12.9782\n",
      "Epoch 38/200\n",
      " - 0s - loss: 6.8724 - val_loss: 13.3709\n",
      "Epoch 39/200\n",
      " - 0s - loss: 6.6851 - val_loss: 11.6605\n",
      "Epoch 40/200\n",
      " - 0s - loss: 6.4911 - val_loss: 13.4436\n",
      "Epoch 41/200\n",
      " - 0s - loss: 6.6104 - val_loss: 12.7878\n",
      "Epoch 42/200\n",
      " - 0s - loss: 6.6853 - val_loss: 12.7766\n",
      "Epoch 43/200\n",
      " - 0s - loss: 6.4900 - val_loss: 12.3650\n",
      "Epoch 44/200\n",
      " - 0s - loss: 6.4937 - val_loss: 12.3790\n",
      "Epoch 45/200\n",
      " - 0s - loss: 6.7982 - val_loss: 13.6318\n",
      "Epoch 46/200\n",
      " - 0s - loss: 6.6052 - val_loss: 11.4533\n",
      "Epoch 47/200\n",
      " - 0s - loss: 6.5178 - val_loss: 12.9566\n",
      "Epoch 48/200\n",
      " - 0s - loss: 6.5377 - val_loss: 15.2077\n",
      "Epoch 49/200\n",
      " - 0s - loss: 6.6862 - val_loss: 12.7813\n",
      "Epoch 50/200\n",
      " - 0s - loss: 6.4842 - val_loss: 12.4046\n",
      "Epoch 51/200\n",
      " - 0s - loss: 6.6982 - val_loss: 12.4406\n",
      "Epoch 52/200\n",
      " - 0s - loss: 6.6819 - val_loss: 14.0503\n",
      "Epoch 53/200\n",
      " - 0s - loss: 6.5371 - val_loss: 12.6426\n",
      "Epoch 54/200\n",
      " - 0s - loss: 6.4402 - val_loss: 12.7317\n",
      "Epoch 55/200\n",
      " - 0s - loss: 6.5249 - val_loss: 13.3903\n",
      "Epoch 56/200\n",
      " - 0s - loss: 6.6721 - val_loss: 12.5003\n",
      "Epoch 57/200\n",
      " - 0s - loss: 6.5310 - val_loss: 14.4070\n",
      "Epoch 58/200\n",
      " - 0s - loss: 6.4580 - val_loss: 12.8940\n",
      "Epoch 59/200\n",
      " - 0s - loss: 6.5500 - val_loss: 12.8630\n",
      "Epoch 60/200\n",
      " - 0s - loss: 6.3979 - val_loss: 13.1941\n",
      "Epoch 61/200\n",
      " - 0s - loss: 6.2936 - val_loss: 13.3980\n",
      "Epoch 62/200\n",
      " - 0s - loss: 6.3246 - val_loss: 13.6264\n",
      "Epoch 63/200\n",
      " - 0s - loss: 6.3715 - val_loss: 14.0615\n",
      "Epoch 64/200\n",
      " - 0s - loss: 6.3434 - val_loss: 11.9267\n",
      "Epoch 65/200\n",
      " - 0s - loss: 6.3151 - val_loss: 13.0903\n",
      "Epoch 66/200\n",
      " - 0s - loss: 6.3726 - val_loss: 12.3785\n",
      "Epoch 67/200\n",
      " - 0s - loss: 6.4525 - val_loss: 12.8362\n",
      "Epoch 68/200\n",
      " - 0s - loss: 6.3785 - val_loss: 12.8349\n",
      "Epoch 69/200\n",
      " - 0s - loss: 6.3169 - val_loss: 12.9012\n",
      "Epoch 70/200\n",
      " - 0s - loss: 6.3092 - val_loss: 13.4696\n",
      "Epoch 71/200\n",
      " - 0s - loss: 6.4153 - val_loss: 12.5304\n",
      "Epoch 72/200\n",
      " - 0s - loss: 6.1936 - val_loss: 13.4970\n",
      "Epoch 73/200\n",
      " - 0s - loss: 6.4604 - val_loss: 12.5845\n",
      "Epoch 74/200\n",
      " - 0s - loss: 6.2938 - val_loss: 14.0791\n",
      "Epoch 75/200\n",
      " - 0s - loss: 6.2905 - val_loss: 13.9050\n",
      "Epoch 76/200\n",
      " - 0s - loss: 6.2584 - val_loss: 13.1217\n",
      "Epoch 77/200\n",
      " - 0s - loss: 6.2894 - val_loss: 13.2484\n",
      "Epoch 78/200\n",
      " - 0s - loss: 6.2513 - val_loss: 12.8393\n",
      "Epoch 79/200\n",
      " - 0s - loss: 6.1021 - val_loss: 12.1482\n",
      "Epoch 80/200\n",
      " - 0s - loss: 6.1914 - val_loss: 14.1528\n",
      "Epoch 81/200\n",
      " - 0s - loss: 6.4212 - val_loss: 12.9309\n",
      "Epoch 82/200\n",
      " - 0s - loss: 6.2902 - val_loss: 12.9429\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.1966 - val_loss: 12.9093\n",
      "Epoch 84/200\n",
      " - 0s - loss: 6.1668 - val_loss: 12.8106\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.2490 - val_loss: 12.9533\n",
      "Epoch 86/200\n",
      " - 0s - loss: 6.0480 - val_loss: 13.5997\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.0872 - val_loss: 12.1690\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.1638 - val_loss: 13.5587\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.3294 - val_loss: 13.2202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200\n",
      " - 0s - loss: 6.0837 - val_loss: 12.4968\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.2367 - val_loss: 12.8599\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.0497 - val_loss: 12.8530\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.0184 - val_loss: 12.0007\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.0278 - val_loss: 12.4129\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.0083 - val_loss: 12.4369\n",
      "Epoch 96/200\n",
      " - 0s - loss: 6.1294 - val_loss: 12.5943\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.0238 - val_loss: 14.0958\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.1227 - val_loss: 13.8043\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.1340 - val_loss: 14.6664\n",
      "Epoch 100/200\n",
      " - 0s - loss: 5.9662 - val_loss: 13.1609\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.0170 - val_loss: 12.0578\n",
      "Epoch 102/200\n",
      " - 0s - loss: 5.9630 - val_loss: 12.3733\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.0017 - val_loss: 13.3104\n",
      "Epoch 104/200\n",
      " - 0s - loss: 5.9754 - val_loss: 12.8469\n",
      "Epoch 105/200\n",
      " - 0s - loss: 5.9077 - val_loss: 12.6155\n",
      "Epoch 106/200\n",
      " - 0s - loss: 5.8725 - val_loss: 14.0466\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.1540 - val_loss: 11.8174\n",
      "Epoch 108/200\n",
      " - 0s - loss: 5.8970 - val_loss: 13.3161\n",
      "Epoch 109/200\n",
      " - 0s - loss: 5.8849 - val_loss: 11.5135\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.0646 - val_loss: 13.2166\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.0676 - val_loss: 12.9698\n",
      "Epoch 112/200\n",
      " - 0s - loss: 5.7962 - val_loss: 12.9231\n",
      "Epoch 113/200\n",
      " - 0s - loss: 5.8314 - val_loss: 13.3494\n",
      "Epoch 114/200\n",
      " - 0s - loss: 5.7860 - val_loss: 12.2479\n",
      "Epoch 115/200\n",
      " - 0s - loss: 5.8346 - val_loss: 13.2657\n",
      "Epoch 116/200\n",
      " - 0s - loss: 5.9253 - val_loss: 13.6740\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.0220 - val_loss: 13.3240\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.0018 - val_loss: 12.0504\n",
      "Epoch 119/200\n",
      " - 0s - loss: 5.8726 - val_loss: 12.7496\n",
      "Epoch 120/200\n",
      " - 0s - loss: 5.9938 - val_loss: 13.0429\n",
      "Epoch 121/200\n",
      " - 0s - loss: 5.8362 - val_loss: 12.2297\n",
      "Epoch 122/200\n",
      " - 0s - loss: 5.8748 - val_loss: 14.0847\n",
      "Epoch 123/200\n",
      " - 0s - loss: 5.8111 - val_loss: 11.7726\n",
      "Epoch 124/200\n",
      " - 0s - loss: 5.9018 - val_loss: 12.1917\n",
      "Epoch 125/200\n",
      " - 0s - loss: 5.8697 - val_loss: 12.0155\n",
      "Epoch 126/200\n",
      " - 0s - loss: 5.8498 - val_loss: 13.2135\n",
      "Epoch 127/200\n",
      " - 0s - loss: 5.5763 - val_loss: 12.4279\n",
      "Epoch 128/200\n",
      " - 0s - loss: 5.6707 - val_loss: 12.2993\n",
      "Epoch 129/200\n",
      " - 0s - loss: 5.7898 - val_loss: 13.3736\n",
      "Epoch 130/200\n",
      " - 0s - loss: 5.6690 - val_loss: 11.8539\n",
      "Epoch 131/200\n",
      " - 0s - loss: 5.9633 - val_loss: 13.1828\n",
      "Epoch 132/200\n",
      " - 0s - loss: 5.8099 - val_loss: 13.2890\n",
      "Epoch 133/200\n",
      " - 0s - loss: 5.8001 - val_loss: 12.7312\n",
      "Epoch 134/200\n",
      " - 0s - loss: 5.7230 - val_loss: 13.2465\n",
      "Epoch 135/200\n",
      " - 0s - loss: 5.7614 - val_loss: 14.2408\n",
      "Epoch 136/200\n",
      " - 0s - loss: 5.7500 - val_loss: 12.8665\n",
      "Epoch 137/200\n",
      " - 0s - loss: 5.7719 - val_loss: 12.4459\n",
      "Epoch 138/200\n",
      " - 0s - loss: 5.6404 - val_loss: 13.9663\n",
      "Epoch 139/200\n",
      " - 0s - loss: 5.7297 - val_loss: 12.6540\n",
      "Epoch 140/200\n",
      " - 0s - loss: 5.8978 - val_loss: 11.7085\n",
      "Epoch 141/200\n",
      " - 0s - loss: 5.7554 - val_loss: 10.3806\n",
      "Epoch 142/200\n",
      " - 0s - loss: 5.6114 - val_loss: 12.0815\n",
      "Epoch 143/200\n",
      " - 0s - loss: 5.6783 - val_loss: 12.7940\n",
      "Epoch 144/200\n",
      " - 0s - loss: 5.6542 - val_loss: 12.3065\n",
      "Epoch 145/200\n",
      " - 0s - loss: 5.6698 - val_loss: 12.5142\n",
      "Epoch 146/200\n",
      " - 0s - loss: 5.6451 - val_loss: 11.5410\n",
      "Epoch 147/200\n",
      " - 0s - loss: 5.6683 - val_loss: 12.2233\n",
      "Epoch 148/200\n",
      " - 0s - loss: 5.4668 - val_loss: 11.8275\n",
      "Epoch 149/200\n",
      " - 0s - loss: 5.6801 - val_loss: 11.8041\n",
      "Epoch 150/200\n",
      " - 0s - loss: 5.7546 - val_loss: 13.6754\n",
      "Epoch 151/200\n",
      " - 0s - loss: 5.6375 - val_loss: 12.4793\n",
      "Epoch 152/200\n",
      " - 0s - loss: 5.5723 - val_loss: 12.6798\n",
      "Epoch 153/200\n",
      " - 0s - loss: 5.6674 - val_loss: 12.8725\n",
      "Epoch 154/200\n",
      " - 0s - loss: 5.4514 - val_loss: 12.6537\n",
      "Epoch 155/200\n",
      " - 0s - loss: 5.6246 - val_loss: 13.2127\n",
      "Epoch 156/200\n",
      " - 0s - loss: 5.6734 - val_loss: 12.8970\n",
      "Epoch 157/200\n",
      " - 0s - loss: 5.5742 - val_loss: 12.9374\n",
      "Epoch 158/200\n",
      " - 0s - loss: 5.8438 - val_loss: 12.8602\n",
      "Epoch 159/200\n",
      " - 0s - loss: 5.5804 - val_loss: 14.8712\n",
      "Epoch 160/200\n",
      " - 0s - loss: 5.5286 - val_loss: 12.8740\n",
      "Epoch 161/200\n",
      " - 0s - loss: 5.4579 - val_loss: 12.0306\n",
      "Epoch 162/200\n",
      " - 0s - loss: 5.6221 - val_loss: 12.9515\n",
      "Epoch 163/200\n",
      " - 0s - loss: 5.6284 - val_loss: 12.3358\n",
      "Epoch 164/200\n",
      " - 0s - loss: 5.4459 - val_loss: 13.4152\n",
      "Epoch 165/200\n",
      " - 0s - loss: 5.4385 - val_loss: 12.8797\n",
      "Epoch 166/200\n",
      " - 0s - loss: 5.4202 - val_loss: 14.5691\n",
      "Epoch 167/200\n",
      " - 0s - loss: 5.5282 - val_loss: 12.8684\n",
      "Epoch 168/200\n",
      " - 0s - loss: 5.7526 - val_loss: 13.0430\n",
      "Epoch 169/200\n",
      " - 0s - loss: 5.7040 - val_loss: 11.9575\n",
      "Epoch 170/200\n",
      " - 0s - loss: 5.5763 - val_loss: 12.9894\n",
      "Epoch 171/200\n",
      " - 0s - loss: 5.5843 - val_loss: 12.6157\n",
      "Epoch 172/200\n",
      " - 0s - loss: 5.4140 - val_loss: 11.8137\n",
      "Epoch 173/200\n",
      " - 0s - loss: 5.3253 - val_loss: 12.9241\n",
      "Epoch 174/200\n",
      " - 0s - loss: 5.4672 - val_loss: 12.6886\n",
      "Epoch 175/200\n",
      " - 0s - loss: 5.5053 - val_loss: 12.8304\n",
      "Epoch 176/200\n",
      " - 0s - loss: 5.8620 - val_loss: 13.8005\n",
      "Epoch 177/200\n",
      " - 0s - loss: 5.5366 - val_loss: 12.9936\n",
      "Epoch 178/200\n",
      " - 0s - loss: 5.5909 - val_loss: 14.3043\n",
      "Epoch 179/200\n",
      " - 0s - loss: 5.4492 - val_loss: 12.8833\n",
      "Epoch 180/200\n",
      " - 0s - loss: 5.5458 - val_loss: 13.6162\n",
      "Epoch 181/200\n",
      " - 0s - loss: 5.3737 - val_loss: 13.4694\n",
      "Epoch 182/200\n",
      " - 0s - loss: 5.5707 - val_loss: 12.9227\n",
      "Epoch 183/200\n",
      " - 0s - loss: 5.4635 - val_loss: 13.2490\n",
      "Epoch 184/200\n",
      " - 0s - loss: 5.5570 - val_loss: 13.6648\n",
      "Epoch 185/200\n",
      " - 0s - loss: 5.6242 - val_loss: 13.9905\n",
      "Epoch 186/200\n",
      " - 0s - loss: 5.4359 - val_loss: 12.7830\n",
      "Epoch 187/200\n",
      " - 0s - loss: 5.5194 - val_loss: 12.9604\n",
      "Epoch 188/200\n",
      " - 0s - loss: 5.4536 - val_loss: 14.8376\n",
      "Epoch 189/200\n",
      " - 0s - loss: 5.5181 - val_loss: 13.3986\n",
      "Epoch 190/200\n",
      " - 0s - loss: 5.1878 - val_loss: 13.8352\n",
      "Epoch 191/200\n",
      " - 0s - loss: 5.3502 - val_loss: 13.1018\n",
      "Epoch 192/200\n",
      " - 0s - loss: 5.1907 - val_loss: 12.1151\n",
      "Epoch 193/200\n",
      " - 0s - loss: 5.2341 - val_loss: 12.5443\n",
      "Epoch 194/200\n",
      " - 0s - loss: 5.1790 - val_loss: 12.7986\n",
      "Epoch 195/200\n",
      " - 0s - loss: 5.3002 - val_loss: 12.5117\n",
      "Epoch 196/200\n",
      " - 0s - loss: 5.2128 - val_loss: 13.0211\n",
      "Epoch 197/200\n",
      " - 0s - loss: 5.2216 - val_loss: 13.5606\n",
      "Epoch 198/200\n",
      " - 0s - loss: 5.4075 - val_loss: 12.4624\n",
      "Epoch 199/200\n",
      " - 0s - loss: 5.2610 - val_loss: 13.3811\n",
      "Epoch 200/200\n",
      " - 0s - loss: 5.4117 - val_loss: 12.9193\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785\n",
      " 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803\n",
      " 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821\n",
      " 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839\n",
      " 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857\n",
      " 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875\n",
      " 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893\n",
      " 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911\n",
      " 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929\n",
      " 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947\n",
      " 948 949 950 951 952 953 954 955 956 957 958 959]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 25.0464 - val_loss: 21.6463\n",
      "Epoch 2/200\n",
      " - 0s - loss: 10.0841 - val_loss: 20.8085\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.0700 - val_loss: 14.2706\n",
      "Epoch 4/200\n",
      " - 0s - loss: 7.8131 - val_loss: 11.7616\n",
      "Epoch 5/200\n",
      " - 0s - loss: 7.7094 - val_loss: 12.7127\n",
      "Epoch 6/200\n",
      " - 0s - loss: 7.5430 - val_loss: 14.6798\n",
      "Epoch 7/200\n",
      " - 0s - loss: 7.5510 - val_loss: 12.8431\n",
      "Epoch 8/200\n",
      " - 0s - loss: 7.6094 - val_loss: 12.1786\n",
      "Epoch 9/200\n",
      " - 0s - loss: 7.5162 - val_loss: 12.2670\n",
      "Epoch 10/200\n",
      " - 0s - loss: 7.5426 - val_loss: 13.2393\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.5571 - val_loss: 12.0600\n",
      "Epoch 12/200\n",
      " - 0s - loss: 7.3763 - val_loss: 12.3641\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.3710 - val_loss: 12.0052\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.3346 - val_loss: 12.3508\n",
      "Epoch 15/200\n",
      " - 0s - loss: 7.3978 - val_loss: 12.6128\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.3892 - val_loss: 12.3603\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.2082 - val_loss: 11.9834\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.1851 - val_loss: 11.9168\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.2338 - val_loss: 12.9786\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.2647 - val_loss: 12.0194\n",
      "Epoch 21/200\n",
      " - 0s - loss: 7.2594 - val_loss: 12.5036\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.2644 - val_loss: 11.9527\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.2398 - val_loss: 12.4860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200\n",
      " - 0s - loss: 7.2311 - val_loss: 13.0447\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.2412 - val_loss: 12.5789\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.2765 - val_loss: 14.2422\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.1871 - val_loss: 11.7910\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.3171 - val_loss: 12.4327\n",
      "Epoch 29/200\n",
      " - 0s - loss: 6.9691 - val_loss: 12.0566\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.1382 - val_loss: 12.0860\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.2142 - val_loss: 11.8901\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.1202 - val_loss: 13.0797\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.1824 - val_loss: 12.1004\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.1437 - val_loss: 12.6310\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.0742 - val_loss: 12.9632\n",
      "Epoch 36/200\n",
      " - 0s - loss: 6.9643 - val_loss: 12.1635\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.0987 - val_loss: 12.5595\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.1600 - val_loss: 12.3929\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.1150 - val_loss: 12.4839\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.0026 - val_loss: 11.9757\n",
      "Epoch 41/200\n",
      " - 0s - loss: 6.9461 - val_loss: 13.2549\n",
      "Epoch 42/200\n",
      " - 0s - loss: 6.9280 - val_loss: 11.8351\n",
      "Epoch 43/200\n",
      " - 0s - loss: 6.9498 - val_loss: 12.3066\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.0860 - val_loss: 12.2922\n",
      "Epoch 45/200\n",
      " - 0s - loss: 6.9049 - val_loss: 11.7173\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.0019 - val_loss: 12.0898\n",
      "Epoch 47/200\n",
      " - 0s - loss: 6.8542 - val_loss: 12.9390\n",
      "Epoch 48/200\n",
      " - 0s - loss: 6.9126 - val_loss: 11.8504\n",
      "Epoch 49/200\n",
      " - 0s - loss: 6.9654 - val_loss: 13.0450\n",
      "Epoch 50/200\n",
      " - 0s - loss: 6.9255 - val_loss: 12.4620\n",
      "Epoch 51/200\n",
      " - 0s - loss: 6.9076 - val_loss: 12.0915\n",
      "Epoch 52/200\n",
      " - 0s - loss: 6.8985 - val_loss: 12.5719\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.0331 - val_loss: 12.3648\n",
      "Epoch 54/200\n",
      " - 0s - loss: 6.8291 - val_loss: 12.3696\n",
      "Epoch 55/200\n",
      " - 0s - loss: 6.7284 - val_loss: 12.6797\n",
      "Epoch 56/200\n",
      " - 0s - loss: 6.8275 - val_loss: 12.0953\n",
      "Epoch 57/200\n",
      " - 0s - loss: 6.9913 - val_loss: 12.0113\n",
      "Epoch 58/200\n",
      " - 0s - loss: 6.8755 - val_loss: 12.2374\n",
      "Epoch 59/200\n",
      " - 0s - loss: 6.7937 - val_loss: 12.8668\n",
      "Epoch 60/200\n",
      " - 0s - loss: 6.9318 - val_loss: 11.9396\n",
      "Epoch 61/200\n",
      " - 0s - loss: 6.6985 - val_loss: 12.5140\n",
      "Epoch 62/200\n",
      " - 0s - loss: 6.7632 - val_loss: 12.3953\n",
      "Epoch 63/200\n",
      " - 0s - loss: 6.8612 - val_loss: 12.1209\n",
      "Epoch 64/200\n",
      " - 0s - loss: 6.8522 - val_loss: 12.2977\n",
      "Epoch 65/200\n",
      " - 0s - loss: 6.6957 - val_loss: 12.0966\n",
      "Epoch 66/200\n",
      " - 0s - loss: 6.8374 - val_loss: 12.7948\n",
      "Epoch 67/200\n",
      " - 0s - loss: 6.6641 - val_loss: 12.7608\n",
      "Epoch 68/200\n",
      " - 0s - loss: 6.6894 - val_loss: 11.5391\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.0347 - val_loss: 12.4877\n",
      "Epoch 70/200\n",
      " - 0s - loss: 6.7320 - val_loss: 12.4234\n",
      "Epoch 71/200\n",
      " - 0s - loss: 6.8556 - val_loss: 12.9010\n",
      "Epoch 72/200\n",
      " - 0s - loss: 6.8591 - val_loss: 11.7669\n",
      "Epoch 73/200\n",
      " - 0s - loss: 6.6146 - val_loss: 12.4264\n",
      "Epoch 74/200\n",
      " - 0s - loss: 6.6063 - val_loss: 12.1333\n",
      "Epoch 75/200\n",
      " - 0s - loss: 6.5862 - val_loss: 12.3891\n",
      "Epoch 76/200\n",
      " - 0s - loss: 6.5423 - val_loss: 12.2360\n",
      "Epoch 77/200\n",
      " - 0s - loss: 6.5000 - val_loss: 11.6582\n",
      "Epoch 78/200\n",
      " - 0s - loss: 6.6792 - val_loss: 12.7602\n",
      "Epoch 79/200\n",
      " - 0s - loss: 6.6444 - val_loss: 12.1413\n",
      "Epoch 80/200\n",
      " - 0s - loss: 6.5704 - val_loss: 12.1432\n",
      "Epoch 81/200\n",
      " - 0s - loss: 6.5829 - val_loss: 12.3365\n",
      "Epoch 82/200\n",
      " - 0s - loss: 6.5122 - val_loss: 12.9702\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.5381 - val_loss: 12.1188\n",
      "Epoch 84/200\n",
      " - 0s - loss: 6.5757 - val_loss: 11.9681\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.4871 - val_loss: 12.0804\n",
      "Epoch 86/200\n",
      " - 0s - loss: 6.5524 - val_loss: 12.0191\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.5952 - val_loss: 12.4976\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.6237 - val_loss: 11.7080\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.5933 - val_loss: 12.8918\n",
      "Epoch 90/200\n",
      " - 0s - loss: 6.7642 - val_loss: 11.7857\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.5459 - val_loss: 13.9974\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.5879 - val_loss: 12.6860\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.4915 - val_loss: 12.8879\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.5221 - val_loss: 11.5562\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.5021 - val_loss: 12.2671\n",
      "Epoch 96/200\n",
      " - 0s - loss: 6.5171 - val_loss: 12.3771\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.5346 - val_loss: 13.2413\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.8578 - val_loss: 12.5204\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.3890 - val_loss: 11.9322\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.3856 - val_loss: 11.7739\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.4455 - val_loss: 12.7072\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.3731 - val_loss: 12.3143\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.3577 - val_loss: 13.3689\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.4106 - val_loss: 11.8281\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.5311 - val_loss: 13.2389\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.3929 - val_loss: 11.3055\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.5392 - val_loss: 12.6649\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.4874 - val_loss: 12.0095\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.3762 - val_loss: 11.9876\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.2420 - val_loss: 12.0601\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.3171 - val_loss: 13.0799\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.4175 - val_loss: 11.6149\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.3826 - val_loss: 12.3070\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.1674 - val_loss: 12.2264\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.4800 - val_loss: 12.3468\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.3750 - val_loss: 15.7906\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.3228 - val_loss: 11.6559\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.4566 - val_loss: 11.8289\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.2301 - val_loss: 12.2594\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.2017 - val_loss: 12.3478\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.1462 - val_loss: 12.0649\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.3286 - val_loss: 12.8537\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.3562 - val_loss: 12.1981\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.2951 - val_loss: 13.5384\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.2464 - val_loss: 12.4513\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.2059 - val_loss: 11.9566\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.2093 - val_loss: 12.9911\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.1997 - val_loss: 12.3649\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.3756 - val_loss: 12.7452\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.4950 - val_loss: 12.5134\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.4222 - val_loss: 11.4096\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.2710 - val_loss: 12.9498\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.2175 - val_loss: 11.9528\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.2352 - val_loss: 12.2936\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.1947 - val_loss: 11.9487\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.1434 - val_loss: 11.9977\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.1609 - val_loss: 12.4979\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.0702 - val_loss: 12.4824\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.1165 - val_loss: 12.6179\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.2218 - val_loss: 12.5929\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.4604 - val_loss: 12.1431\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.1450 - val_loss: 12.7667\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.1763 - val_loss: 12.0169\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.1215 - val_loss: 12.4058\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.3853 - val_loss: 12.6424\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.2751 - val_loss: 13.4884\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.2601 - val_loss: 12.3322\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.0721 - val_loss: 12.3986\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.1827 - val_loss: 13.0244\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.1656 - val_loss: 12.9155\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.1785 - val_loss: 12.0140\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.2565 - val_loss: 12.2535\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.0548 - val_loss: 13.1613\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.2113 - val_loss: 11.5571\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.0872 - val_loss: 13.0012\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.2632 - val_loss: 12.2633\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.0612 - val_loss: 12.3273\n",
      "Epoch 158/200\n",
      " - 0s - loss: 5.9545 - val_loss: 12.3663\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.0749 - val_loss: 11.6424\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.3421 - val_loss: 11.8851\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.1651 - val_loss: 11.4392\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.0491 - val_loss: 12.5548\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.1153 - val_loss: 12.0145\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.0006 - val_loss: 12.8921\n",
      "Epoch 165/200\n",
      " - 0s - loss: 5.9936 - val_loss: 12.9349\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.0232 - val_loss: 12.2116\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.0464 - val_loss: 12.2137\n",
      "Epoch 168/200\n",
      " - 0s - loss: 5.9103 - val_loss: 12.3661\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.0985 - val_loss: 11.7052\n",
      "Epoch 170/200\n",
      " - 0s - loss: 5.9956 - val_loss: 13.0178\n",
      "Epoch 171/200\n",
      " - 0s - loss: 5.8828 - val_loss: 12.4140\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.1276 - val_loss: 12.0585\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.1662 - val_loss: 12.5281\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.0377 - val_loss: 11.7347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      " - 0s - loss: 6.0989 - val_loss: 12.2575\n",
      "Epoch 176/200\n",
      " - 0s - loss: 5.8951 - val_loss: 12.7480\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.0041 - val_loss: 11.6384\n",
      "Epoch 178/200\n",
      " - 0s - loss: 5.9420 - val_loss: 11.9098\n",
      "Epoch 179/200\n",
      " - 0s - loss: 5.8495 - val_loss: 11.7617\n",
      "Epoch 180/200\n",
      " - 0s - loss: 5.9798 - val_loss: 12.3031\n",
      "Epoch 181/200\n",
      " - 0s - loss: 5.9568 - val_loss: 12.2657\n",
      "Epoch 182/200\n",
      " - 0s - loss: 5.9485 - val_loss: 12.4573\n",
      "Epoch 183/200\n",
      " - 0s - loss: 5.8110 - val_loss: 11.4038\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.2296 - val_loss: 11.8820\n",
      "Epoch 185/200\n",
      " - 0s - loss: 5.9680 - val_loss: 12.4554\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.0679 - val_loss: 12.1704\n",
      "Epoch 187/200\n",
      " - 0s - loss: 5.9110 - val_loss: 12.6117\n",
      "Epoch 188/200\n",
      " - 0s - loss: 5.9928 - val_loss: 12.5341\n",
      "Epoch 189/200\n",
      " - 0s - loss: 5.8705 - val_loss: 13.1649\n",
      "Epoch 190/200\n",
      " - 0s - loss: 5.7770 - val_loss: 13.6482\n",
      "Epoch 191/200\n",
      " - 0s - loss: 5.9679 - val_loss: 12.9183\n",
      "Epoch 192/200\n",
      " - 0s - loss: 5.8565 - val_loss: 13.2197\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.1345 - val_loss: 12.3809\n",
      "Epoch 194/200\n",
      " - 0s - loss: 5.9049 - val_loss: 12.3247\n",
      "Epoch 195/200\n",
      " - 0s - loss: 5.7952 - val_loss: 12.3014\n",
      "Epoch 196/200\n",
      " - 0s - loss: 5.6707 - val_loss: 12.2751\n",
      "Epoch 197/200\n",
      " - 0s - loss: 5.7429 - val_loss: 12.2131\n",
      "Epoch 198/200\n",
      " - 0s - loss: 5.8901 - val_loss: 11.9766\n",
      "Epoch 199/200\n",
      " - 0s - loss: 5.8258 - val_loss: 12.7939\n",
      "Epoch 200/200\n",
      " - 0s - loss: 5.9309 - val_loss: 12.0185\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [ 960  961  962  963  964  965  966  967  968  969  970  971  972  973\n",
      "  974  975  976  977  978  979  980  981  982  983  984  985  986  987\n",
      "  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001\n",
      " 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n",
      " 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029\n",
      " 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043\n",
      " 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057\n",
      " 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071\n",
      " 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085\n",
      " 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099\n",
      " 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113\n",
      " 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127\n",
      " 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141\n",
      " 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151]\n",
      "Train on 1379 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 23.9904 - val_loss: 19.0447\n",
      "Epoch 2/200\n",
      " - 0s - loss: 9.9880 - val_loss: 13.5415\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.4961 - val_loss: 12.4572\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.5315 - val_loss: 14.7703\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.2498 - val_loss: 12.8693\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.2452 - val_loss: 14.8993\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.1416 - val_loss: 11.9733\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.1699 - val_loss: 12.9750\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.0603 - val_loss: 12.8656\n",
      "Epoch 10/200\n",
      " - 0s - loss: 8.0105 - val_loss: 12.7042\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.9895 - val_loss: 12.1892\n",
      "Epoch 12/200\n",
      " - 0s - loss: 7.9920 - val_loss: 12.4801\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.9575 - val_loss: 12.6154\n",
      "Epoch 14/200\n",
      " - 0s - loss: 8.0002 - val_loss: 12.1480\n",
      "Epoch 15/200\n",
      " - 0s - loss: 8.0329 - val_loss: 12.8605\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.9071 - val_loss: 12.9666\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.9030 - val_loss: 11.7989\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.8380 - val_loss: 12.2627\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.9021 - val_loss: 11.5543\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.9523 - val_loss: 12.5136\n",
      "Epoch 21/200\n",
      " - 0s - loss: 7.9037 - val_loss: 12.6705\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.8412 - val_loss: 11.7745\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.8344 - val_loss: 12.6941\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.8362 - val_loss: 12.7121\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.6950 - val_loss: 12.6484\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.7650 - val_loss: 12.5162\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.6030 - val_loss: 12.8930\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.6689 - val_loss: 13.1370\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.5692 - val_loss: 11.4176\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.7411 - val_loss: 12.3224\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.6268 - val_loss: 12.7780\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.6639 - val_loss: 11.8122\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.5455 - val_loss: 12.1971\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.7160 - val_loss: 11.7868\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.5084 - val_loss: 12.5473\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.7118 - val_loss: 11.9676\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.7402 - val_loss: 11.9865\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.5301 - val_loss: 12.8357\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.5536 - val_loss: 12.4475\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.5975 - val_loss: 12.8830\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.5177 - val_loss: 13.1765\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.4889 - val_loss: 12.0806\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.4927 - val_loss: 13.2833\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.4746 - val_loss: 12.6252\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.4319 - val_loss: 12.6009\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.2827 - val_loss: 11.4299\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.3877 - val_loss: 12.3419\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.3689 - val_loss: 13.0023\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.4570 - val_loss: 12.6867\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.3780 - val_loss: 11.6838\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.2402 - val_loss: 11.9071\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.4134 - val_loss: 12.5489\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.4349 - val_loss: 12.6279\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.3102 - val_loss: 12.7303\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.4477 - val_loss: 12.5721\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.2138 - val_loss: 12.8424\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.2368 - val_loss: 12.3076\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.2721 - val_loss: 12.2862\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.2249 - val_loss: 11.9168\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.4927 - val_loss: 11.4142\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.2046 - val_loss: 14.0757\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.4155 - val_loss: 12.3719\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.2604 - val_loss: 12.8720\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.2727 - val_loss: 12.7116\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.1302 - val_loss: 11.5465\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.1991 - val_loss: 12.5744\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.2143 - val_loss: 13.1355\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.1708 - val_loss: 11.4827\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.2739 - val_loss: 12.8860\n",
      "Epoch 70/200\n",
      " - 0s - loss: 7.1742 - val_loss: 13.1621\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.2185 - val_loss: 13.9575\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.0687 - val_loss: 12.4189\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.3154 - val_loss: 12.6828\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.1724 - val_loss: 13.7779\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.1681 - val_loss: 11.3239\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.0465 - val_loss: 12.9086\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.1532 - val_loss: 12.1049\n",
      "Epoch 78/200\n",
      " - 0s - loss: 7.0270 - val_loss: 12.4099\n",
      "Epoch 79/200\n",
      " - 0s - loss: 7.0912 - val_loss: 11.6951\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.0954 - val_loss: 12.2115\n",
      "Epoch 81/200\n",
      " - 0s - loss: 7.0008 - val_loss: 12.4417\n",
      "Epoch 82/200\n",
      " - 0s - loss: 6.9376 - val_loss: 12.8389\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.9766 - val_loss: 12.9090\n",
      "Epoch 84/200\n",
      " - 0s - loss: 7.2146 - val_loss: 11.9638\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.9873 - val_loss: 13.7481\n",
      "Epoch 86/200\n",
      " - 0s - loss: 7.0132 - val_loss: 12.0921\n",
      "Epoch 87/200\n",
      " - 0s - loss: 7.0608 - val_loss: 12.2045\n",
      "Epoch 88/200\n",
      " - 0s - loss: 7.1098 - val_loss: 12.2512\n",
      "Epoch 89/200\n",
      " - 0s - loss: 7.1005 - val_loss: 11.5681\n",
      "Epoch 90/200\n",
      " - 0s - loss: 6.8026 - val_loss: 13.0461\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.8785 - val_loss: 13.1240\n",
      "Epoch 92/200\n",
      " - 0s - loss: 7.0795 - val_loss: 11.8115\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.8646 - val_loss: 12.6141\n",
      "Epoch 94/200\n",
      " - 0s - loss: 7.0373 - val_loss: 12.1052\n",
      "Epoch 95/200\n",
      " - 0s - loss: 7.0135 - val_loss: 12.9285\n",
      "Epoch 96/200\n",
      " - 0s - loss: 7.0788 - val_loss: 12.4383\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.9509 - val_loss: 12.9573\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.9692 - val_loss: 11.8249\n",
      "Epoch 99/200\n",
      " - 0s - loss: 7.0201 - val_loss: 13.0249\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.8443 - val_loss: 12.5521\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.9166 - val_loss: 11.9621\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.9019 - val_loss: 12.3339\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.9817 - val_loss: 12.6565\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.8168 - val_loss: 13.0347\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.8085 - val_loss: 12.8097\n",
      "Epoch 106/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 6.8631 - val_loss: 11.9542\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.8620 - val_loss: 12.9956\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.7963 - val_loss: 11.5244\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.8201 - val_loss: 13.0222\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.7372 - val_loss: 12.1517\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.9198 - val_loss: 12.2553\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.7578 - val_loss: 12.6641\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.9339 - val_loss: 13.3418\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.8161 - val_loss: 12.5118\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.6341 - val_loss: 12.2965\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.7710 - val_loss: 11.7295\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.7520 - val_loss: 12.6662\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.8836 - val_loss: 13.1082\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.7558 - val_loss: 12.4041\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.5521 - val_loss: 12.4315\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.8345 - val_loss: 11.5666\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.7233 - val_loss: 13.3377\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.5977 - val_loss: 12.6756\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.7938 - val_loss: 12.4766\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.6883 - val_loss: 12.7771\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.8300 - val_loss: 13.3246\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.7760 - val_loss: 13.2490\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.6278 - val_loss: 12.8194\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.6035 - val_loss: 12.9137\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.6327 - val_loss: 12.5468\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.6944 - val_loss: 12.9882\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.6240 - val_loss: 12.2134\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.6932 - val_loss: 12.0832\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.7419 - val_loss: 12.9462\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.5493 - val_loss: 11.3853\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.9462 - val_loss: 12.3058\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.8716 - val_loss: 12.7407\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.6508 - val_loss: 12.1230\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.5545 - val_loss: 13.1592\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.6113 - val_loss: 11.9405\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.6343 - val_loss: 12.1995\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.4745 - val_loss: 13.8379\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.3598 - val_loss: 12.2925\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.5827 - val_loss: 12.7397\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.6026 - val_loss: 11.7556\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.5333 - val_loss: 12.1620\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.6622 - val_loss: 12.9861\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.5200 - val_loss: 12.3198\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.5367 - val_loss: 13.2788\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.5560 - val_loss: 12.9875\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.3551 - val_loss: 12.1374\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.7176 - val_loss: 13.0605\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.5624 - val_loss: 13.1033\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.6523 - val_loss: 12.7113\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.4951 - val_loss: 12.3752\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.4990 - val_loss: 12.7488\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.4804 - val_loss: 13.3304\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.3334 - val_loss: 13.5933\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.4764 - val_loss: 13.1188\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.5549 - val_loss: 12.5567\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.6269 - val_loss: 11.8652\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.5546 - val_loss: 13.1535\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.3975 - val_loss: 11.6370\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.5555 - val_loss: 12.2067\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.4032 - val_loss: 12.6906\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.3501 - val_loss: 12.8623\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.7201 - val_loss: 12.1775\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.2024 - val_loss: 12.6288\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.3347 - val_loss: 12.7612\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.3403 - val_loss: 12.6572\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.4989 - val_loss: 12.4951\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.5137 - val_loss: 11.9191\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.4467 - val_loss: 12.9409\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.3721 - val_loss: 13.2158\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.2911 - val_loss: 12.3576\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.4944 - val_loss: 12.3001\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.2656 - val_loss: 14.3571\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.3313 - val_loss: 12.1751\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.6243 - val_loss: 13.4958\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.1170 - val_loss: 12.5043\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.2628 - val_loss: 13.3915\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.2014 - val_loss: 12.8951\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.3453 - val_loss: 13.1009\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.2634 - val_loss: 13.4806\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.3256 - val_loss: 12.7714\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.4618 - val_loss: 13.0361\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.1470 - val_loss: 14.0324\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.3196 - val_loss: 12.5353\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.4493 - val_loss: 12.6135\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.1778 - val_loss: 13.1660\n",
      "Epoch 191/200\n",
      " - 0s - loss: 6.2736 - val_loss: 12.4964\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.2845 - val_loss: 12.7544\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.2257 - val_loss: 13.3009\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.5581 - val_loss: 12.9908\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.5797 - val_loss: 14.0541\n",
      "Epoch 196/200\n",
      " - 0s - loss: 6.3854 - val_loss: 12.6025\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.2079 - val_loss: 13.4115\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.5580 - val_loss: 13.2672\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.2800 - val_loss: 12.9036\n",
      "Epoch 200/200\n",
      " - 0s - loss: 6.2978 - val_loss: 12.3215\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165\n",
      " 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179\n",
      " 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193\n",
      " 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207\n",
      " 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221\n",
      " 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235\n",
      " 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249\n",
      " 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263\n",
      " 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277\n",
      " 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291\n",
      " 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305\n",
      " 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319\n",
      " 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333\n",
      " 1334 1335 1336 1337 1338 1339 1340 1341 1342]\n",
      "Train on 1380 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 22.3376 - val_loss: 16.4445\n",
      "Epoch 2/200\n",
      " - 0s - loss: 8.8853 - val_loss: 13.9235\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.3555 - val_loss: 12.8372\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.2454 - val_loss: 12.6300\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.1095 - val_loss: 13.2893\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.0552 - val_loss: 12.9886\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.0182 - val_loss: 13.2443\n",
      "Epoch 8/200\n",
      " - 0s - loss: 7.9059 - val_loss: 12.8418\n",
      "Epoch 9/200\n",
      " - 0s - loss: 7.8610 - val_loss: 12.9544\n",
      "Epoch 10/200\n",
      " - 0s - loss: 7.8516 - val_loss: 12.8640\n",
      "Epoch 11/200\n",
      " - 0s - loss: 7.9465 - val_loss: 12.7909\n",
      "Epoch 12/200\n",
      " - 0s - loss: 7.9258 - val_loss: 12.4491\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.9692 - val_loss: 12.4279\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.6030 - val_loss: 12.5675\n",
      "Epoch 15/200\n",
      " - 0s - loss: 7.6350 - val_loss: 13.4654\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.6944 - val_loss: 12.6295\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.6085 - val_loss: 13.1857\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.6047 - val_loss: 13.2447\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.7751 - val_loss: 12.4466\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.5752 - val_loss: 12.3830\n",
      "Epoch 21/200\n",
      " - 0s - loss: 7.6519 - val_loss: 13.0498\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.5853 - val_loss: 13.0297\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.5890 - val_loss: 12.2498\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.4528 - val_loss: 13.0211\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.4295 - val_loss: 13.7384\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.5871 - val_loss: 12.2048\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.4418 - val_loss: 13.0397\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.3433 - val_loss: 12.7902\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.4600 - val_loss: 12.7435\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.3172 - val_loss: 13.2316\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.4302 - val_loss: 13.3550\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.3694 - val_loss: 12.5302\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.2669 - val_loss: 12.3202\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.3150 - val_loss: 13.0752\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.3118 - val_loss: 13.1826\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.3128 - val_loss: 12.8833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      " - 0s - loss: 7.3427 - val_loss: 13.1647\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.2261 - val_loss: 12.9280\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.2467 - val_loss: 13.1200\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.2959 - val_loss: 13.2809\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.2773 - val_loss: 12.6758\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.1174 - val_loss: 13.1001\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.3799 - val_loss: 13.5024\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.2312 - val_loss: 12.7899\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.1355 - val_loss: 12.3319\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.1524 - val_loss: 12.7600\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.1900 - val_loss: 13.5707\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.2036 - val_loss: 12.3882\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.2514 - val_loss: 13.2190\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.2301 - val_loss: 12.3264\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.2065 - val_loss: 13.5145\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.1561 - val_loss: 12.8256\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.2228 - val_loss: 12.6727\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.0162 - val_loss: 13.4060\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.0163 - val_loss: 13.3974\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.1275 - val_loss: 12.7945\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.0301 - val_loss: 13.7537\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.0560 - val_loss: 13.1445\n",
      "Epoch 59/200\n",
      " - 0s - loss: 6.8951 - val_loss: 12.3797\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.2577 - val_loss: 12.2499\n",
      "Epoch 61/200\n",
      " - 0s - loss: 6.9860 - val_loss: 12.7194\n",
      "Epoch 62/200\n",
      " - 0s - loss: 6.8233 - val_loss: 12.8220\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.0223 - val_loss: 13.3480\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.0502 - val_loss: 12.4787\n",
      "Epoch 65/200\n",
      " - 0s - loss: 6.9078 - val_loss: 12.6653\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.0549 - val_loss: 12.3670\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.1045 - val_loss: 12.0780\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.1042 - val_loss: 12.3483\n",
      "Epoch 69/200\n",
      " - 0s - loss: 6.8947 - val_loss: 12.8518\n",
      "Epoch 70/200\n",
      " - 0s - loss: 6.9430 - val_loss: 13.3001\n",
      "Epoch 71/200\n",
      " - 0s - loss: 6.9796 - val_loss: 12.7619\n",
      "Epoch 72/200\n",
      " - 0s - loss: 6.8843 - val_loss: 12.8918\n",
      "Epoch 73/200\n",
      " - 0s - loss: 6.8834 - val_loss: 13.2507\n",
      "Epoch 74/200\n",
      " - 0s - loss: 6.7526 - val_loss: 12.9963\n",
      "Epoch 75/200\n",
      " - 0s - loss: 6.8287 - val_loss: 13.2132\n",
      "Epoch 76/200\n",
      " - 0s - loss: 6.9110 - val_loss: 12.4707\n",
      "Epoch 77/200\n",
      " - 0s - loss: 6.8591 - val_loss: 13.3301\n",
      "Epoch 78/200\n",
      " - 0s - loss: 6.8299 - val_loss: 12.5875\n",
      "Epoch 79/200\n",
      " - 0s - loss: 6.8419 - val_loss: 13.8161\n",
      "Epoch 80/200\n",
      " - 0s - loss: 6.9714 - val_loss: 13.8028\n",
      "Epoch 81/200\n",
      " - 0s - loss: 6.8331 - val_loss: 13.0707\n",
      "Epoch 82/200\n",
      " - 0s - loss: 6.7314 - val_loss: 12.5430\n",
      "Epoch 83/200\n",
      " - 0s - loss: 6.8973 - val_loss: 13.3009\n",
      "Epoch 84/200\n",
      " - 0s - loss: 6.8018 - val_loss: 13.2789\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.7497 - val_loss: 12.7581\n",
      "Epoch 86/200\n",
      " - 0s - loss: 6.8168 - val_loss: 13.2784\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.7016 - val_loss: 12.3837\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.7629 - val_loss: 13.7898\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.6737 - val_loss: 13.1907\n",
      "Epoch 90/200\n",
      " - 0s - loss: 6.6135 - val_loss: 13.5966\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.8196 - val_loss: 11.8581\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.9030 - val_loss: 13.4560\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.8207 - val_loss: 12.0196\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.6274 - val_loss: 12.9149\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.7162 - val_loss: 12.6332\n",
      "Epoch 96/200\n",
      " - 0s - loss: 6.6926 - val_loss: 13.6105\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.9782 - val_loss: 12.7936\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.7010 - val_loss: 13.2227\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.6231 - val_loss: 13.9635\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.7182 - val_loss: 12.0381\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.6906 - val_loss: 12.8128\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.5786 - val_loss: 13.2929\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.5034 - val_loss: 13.0750\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.5097 - val_loss: 12.2977\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.5122 - val_loss: 13.4199\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.7195 - val_loss: 14.0907\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.5405 - val_loss: 13.2673\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.3942 - val_loss: 12.8045\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.5395 - val_loss: 12.8522\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.4519 - val_loss: 13.3518\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.5743 - val_loss: 12.9031\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.5001 - val_loss: 14.0302\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.5737 - val_loss: 13.4964\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.4918 - val_loss: 13.2053\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.5445 - val_loss: 13.3220\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.5855 - val_loss: 13.7713\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.5237 - val_loss: 12.2587\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.5121 - val_loss: 12.8870\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.3637 - val_loss: 13.2366\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.3663 - val_loss: 13.1999\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.4897 - val_loss: 12.5932\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.4221 - val_loss: 13.8314\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.4532 - val_loss: 12.6607\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.4970 - val_loss: 13.4190\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.7161 - val_loss: 13.7865\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.4512 - val_loss: 14.1506\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.3560 - val_loss: 13.1787\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.4426 - val_loss: 12.9937\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.5505 - val_loss: 12.2128\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.3724 - val_loss: 12.6160\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.4710 - val_loss: 13.0859\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.2225 - val_loss: 12.5934\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.3172 - val_loss: 14.2283\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.5840 - val_loss: 13.3590\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.3006 - val_loss: 12.1607\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.5675 - val_loss: 14.1565\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.3573 - val_loss: 14.3958\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.3417 - val_loss: 13.1916\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.4069 - val_loss: 12.8379\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.3203 - val_loss: 13.2122\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.3076 - val_loss: 13.2684\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.4719 - val_loss: 13.1449\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.4017 - val_loss: 12.7814\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.3236 - val_loss: 12.8291\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.2565 - val_loss: 13.0029\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.2027 - val_loss: 12.9987\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.2933 - val_loss: 12.3766\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.1976 - val_loss: 13.0933\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.3077 - val_loss: 13.7040\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.1512 - val_loss: 13.6005\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.2728 - val_loss: 14.0893\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.2595 - val_loss: 13.2486\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.2624 - val_loss: 13.5250\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.3497 - val_loss: 12.9408\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.2562 - val_loss: 14.1452\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.2676 - val_loss: 14.4128\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.0394 - val_loss: 12.6894\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.3038 - val_loss: 12.9833\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.2099 - val_loss: 13.5196\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.2673 - val_loss: 13.6694\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.2450 - val_loss: 13.3162\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.3114 - val_loss: 13.3944\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.1569 - val_loss: 13.7788\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.1677 - val_loss: 14.1278\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.2706 - val_loss: 13.3846\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.0612 - val_loss: 13.1299\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.1257 - val_loss: 14.0527\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.2070 - val_loss: 13.8513\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.2191 - val_loss: 12.3584\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.1096 - val_loss: 12.6582\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.1212 - val_loss: 13.4643\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.1429 - val_loss: 12.4789\n",
      "Epoch 173/200\n",
      " - 0s - loss: 5.9739 - val_loss: 13.3426\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.1165 - val_loss: 14.0737\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.0194 - val_loss: 13.5235\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.0879 - val_loss: 13.7896\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.0608 - val_loss: 13.4166\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.0860 - val_loss: 13.7832\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.0266 - val_loss: 13.0446\n",
      "Epoch 180/200\n",
      " - 0s - loss: 5.9370 - val_loss: 13.8177\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.1400 - val_loss: 14.4624\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.0114 - val_loss: 13.0742\n",
      "Epoch 183/200\n",
      " - 0s - loss: 5.9366 - val_loss: 13.0328\n",
      "Epoch 184/200\n",
      " - 0s - loss: 5.9275 - val_loss: 12.5358\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.2410 - val_loss: 13.7754\n",
      "Epoch 186/200\n",
      " - 0s - loss: 5.9267 - val_loss: 13.3920\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 6.0518 - val_loss: 13.4505\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.0162 - val_loss: 13.4757\n",
      "Epoch 189/200\n",
      " - 0s - loss: 5.9996 - val_loss: 13.1747\n",
      "Epoch 190/200\n",
      " - 0s - loss: 5.7842 - val_loss: 13.7220\n",
      "Epoch 191/200\n",
      " - 0s - loss: 5.8237 - val_loss: 13.4115\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.0461 - val_loss: 12.8323\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.3207 - val_loss: 12.8130\n",
      "Epoch 194/200\n",
      " - 0s - loss: 5.8272 - val_loss: 13.3087\n",
      "Epoch 195/200\n",
      " - 0s - loss: 5.8903 - val_loss: 13.4793\n",
      "Epoch 196/200\n",
      " - 0s - loss: 5.7323 - val_loss: 13.0458\n",
      "Epoch 197/200\n",
      " - 0s - loss: 5.8431 - val_loss: 13.7121\n",
      "Epoch 198/200\n",
      " - 0s - loss: 5.7617 - val_loss: 12.7053\n",
      "Epoch 199/200\n",
      " - 0s - loss: 5.8087 - val_loss: 13.5266\n",
      "Epoch 200/200\n",
      " - 0s - loss: 5.9911 - val_loss: 13.5266\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356\n",
      " 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370\n",
      " 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384\n",
      " 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398\n",
      " 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412\n",
      " 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426\n",
      " 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440\n",
      " 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454\n",
      " 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468\n",
      " 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482\n",
      " 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496\n",
      " 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510\n",
      " 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524\n",
      " 1525 1526 1527 1528 1529 1530 1531 1532 1533]\n",
      "Train on 1380 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 23.8920 - val_loss: 15.2118\n",
      "Epoch 2/200\n",
      " - 0s - loss: 10.3269 - val_loss: 18.8232\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.6585 - val_loss: 14.3799\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.5489 - val_loss: 12.7651\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.3044 - val_loss: 11.8489\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.3904 - val_loss: 12.3102\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.2173 - val_loss: 12.6464\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.2083 - val_loss: 12.9694\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.1918 - val_loss: 12.3879\n",
      "Epoch 10/200\n",
      " - 0s - loss: 8.1719 - val_loss: 12.6087\n",
      "Epoch 11/200\n",
      " - 0s - loss: 8.1119 - val_loss: 13.9092\n",
      "Epoch 12/200\n",
      " - 0s - loss: 8.2466 - val_loss: 12.6632\n",
      "Epoch 13/200\n",
      " - 0s - loss: 8.1267 - val_loss: 12.2183\n",
      "Epoch 14/200\n",
      " - 0s - loss: 7.9299 - val_loss: 12.4006\n",
      "Epoch 15/200\n",
      " - 0s - loss: 8.0503 - val_loss: 12.0914\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.9135 - val_loss: 12.8155\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.9408 - val_loss: 11.9178\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.8324 - val_loss: 11.9382\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.9420 - val_loss: 11.9146\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.8407 - val_loss: 12.0994\n",
      "Epoch 21/200\n",
      " - 0s - loss: 7.8302 - val_loss: 12.6361\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.9924 - val_loss: 12.7170\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.8043 - val_loss: 12.6013\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.8126 - val_loss: 12.6903\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.6637 - val_loss: 12.0288\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.7790 - val_loss: 12.0570\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.6896 - val_loss: 12.1359\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.6866 - val_loss: 12.6879\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.6839 - val_loss: 12.3873\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.5832 - val_loss: 12.5211\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.5989 - val_loss: 12.7098\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.6473 - val_loss: 12.1920\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.7842 - val_loss: 12.4361\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.5358 - val_loss: 12.6684\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.5977 - val_loss: 12.2744\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.5420 - val_loss: 12.0042\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.6885 - val_loss: 12.0749\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.5602 - val_loss: 13.0193\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.7187 - val_loss: 12.8849\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.5771 - val_loss: 12.9178\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.6305 - val_loss: 11.5839\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.4706 - val_loss: 11.6597\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.4233 - val_loss: 12.2435\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.5131 - val_loss: 12.4716\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.4699 - val_loss: 12.3458\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.4572 - val_loss: 12.8112\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.7412 - val_loss: 12.5710\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.5498 - val_loss: 13.6969\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.5329 - val_loss: 13.0378\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.3264 - val_loss: 12.2643\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.4049 - val_loss: 12.6457\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.3276 - val_loss: 12.4491\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.4329 - val_loss: 12.1283\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.3022 - val_loss: 12.8063\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.3629 - val_loss: 12.3589\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.2823 - val_loss: 12.3807\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.2092 - val_loss: 12.2745\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.2919 - val_loss: 12.6928\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.4068 - val_loss: 12.7271\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.2336 - val_loss: 12.3295\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.1569 - val_loss: 11.7908\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.3351 - val_loss: 12.2383\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.2786 - val_loss: 12.9278\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.1830 - val_loss: 12.7453\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.0908 - val_loss: 12.8329\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.2028 - val_loss: 11.9292\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.2965 - val_loss: 11.7879\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.1584 - val_loss: 12.3296\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.1011 - val_loss: 12.7203\n",
      "Epoch 70/200\n",
      " - 0s - loss: 7.2907 - val_loss: 12.1424\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.1553 - val_loss: 12.6168\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.1503 - val_loss: 12.7266\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.2147 - val_loss: 12.1668\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.1465 - val_loss: 12.0555\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.0706 - val_loss: 12.4181\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.1084 - val_loss: 11.8955\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.0783 - val_loss: 12.2938\n",
      "Epoch 78/200\n",
      " - 0s - loss: 6.9176 - val_loss: 12.3486\n",
      "Epoch 79/200\n",
      " - 0s - loss: 7.0331 - val_loss: 12.5981\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.0384 - val_loss: 11.6717\n",
      "Epoch 81/200\n",
      " - 0s - loss: 6.9479 - val_loss: 12.7266\n",
      "Epoch 82/200\n",
      " - 0s - loss: 7.1098 - val_loss: 11.7985\n",
      "Epoch 83/200\n",
      " - 0s - loss: 7.1776 - val_loss: 12.7227\n",
      "Epoch 84/200\n",
      " - 0s - loss: 7.0461 - val_loss: 12.0230\n",
      "Epoch 85/200\n",
      " - 0s - loss: 6.9086 - val_loss: 12.9341\n",
      "Epoch 86/200\n",
      " - 0s - loss: 7.0894 - val_loss: 12.0368\n",
      "Epoch 87/200\n",
      " - 0s - loss: 7.1213 - val_loss: 12.4735\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.9676 - val_loss: 13.0329\n",
      "Epoch 89/200\n",
      " - 0s - loss: 7.2054 - val_loss: 12.3073\n",
      "Epoch 90/200\n",
      " - 0s - loss: 7.0297 - val_loss: 12.6486\n",
      "Epoch 91/200\n",
      " - 0s - loss: 7.0892 - val_loss: 13.4754\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.9721 - val_loss: 13.2409\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.8589 - val_loss: 11.9777\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.9171 - val_loss: 12.5548\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.9998 - val_loss: 12.2471\n",
      "Epoch 96/200\n",
      " - 0s - loss: 7.0000 - val_loss: 13.1351\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.9930 - val_loss: 12.8903\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.8966 - val_loss: 12.1689\n",
      "Epoch 99/200\n",
      " - 0s - loss: 7.0070 - val_loss: 12.6889\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.8236 - val_loss: 12.6692\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.9907 - val_loss: 12.7793\n",
      "Epoch 102/200\n",
      " - 0s - loss: 7.0075 - val_loss: 12.7974\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.8502 - val_loss: 12.3320\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.8519 - val_loss: 13.1613\n",
      "Epoch 105/200\n",
      " - 0s - loss: 7.0229 - val_loss: 13.0789\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.9669 - val_loss: 12.1755\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.9134 - val_loss: 12.7469\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.8308 - val_loss: 12.2781\n",
      "Epoch 109/200\n",
      " - 0s - loss: 7.0066 - val_loss: 13.1372\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.7261 - val_loss: 12.7352\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.6998 - val_loss: 13.0558\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.7998 - val_loss: 13.2120\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.7838 - val_loss: 11.8584\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.7481 - val_loss: 12.6624\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.7960 - val_loss: 12.3358\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.8791 - val_loss: 13.7189\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.7953 - val_loss: 12.4565\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.8445 - val_loss: 11.8878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      " - 0s - loss: 6.7990 - val_loss: 13.6959\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.9014 - val_loss: 12.6432\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.8876 - val_loss: 12.8063\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.7960 - val_loss: 12.5268\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.7214 - val_loss: 11.9004\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.9254 - val_loss: 12.8531\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.7869 - val_loss: 12.8330\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.6693 - val_loss: 12.7014\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.8754 - val_loss: 12.5478\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.8939 - val_loss: 12.8581\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.6950 - val_loss: 12.6186\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.7052 - val_loss: 12.8729\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.7417 - val_loss: 13.5034\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.6853 - val_loss: 13.8470\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.7786 - val_loss: 13.1902\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.7427 - val_loss: 13.0406\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.8694 - val_loss: 12.3797\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.6294 - val_loss: 11.9957\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.6342 - val_loss: 13.3843\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.6538 - val_loss: 12.9122\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.6144 - val_loss: 11.9498\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.6527 - val_loss: 12.2344\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.6248 - val_loss: 12.5412\n",
      "Epoch 142/200\n",
      " - 0s - loss: 7.0075 - val_loss: 13.0296\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.5363 - val_loss: 13.6033\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.9132 - val_loss: 14.7827\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.6769 - val_loss: 13.0121\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.6388 - val_loss: 11.8873\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.6520 - val_loss: 12.4869\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.7618 - val_loss: 12.8849\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.6717 - val_loss: 12.7588\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.4169 - val_loss: 13.1603\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.7102 - val_loss: 12.3276\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.7314 - val_loss: 12.4421\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.5897 - val_loss: 13.3066\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.6555 - val_loss: 12.8427\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.5661 - val_loss: 12.6739\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.6186 - val_loss: 12.3483\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.5574 - val_loss: 12.4083\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.5802 - val_loss: 12.9373\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.4442 - val_loss: 11.7381\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.4603 - val_loss: 12.8053\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.6247 - val_loss: 13.0172\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.5291 - val_loss: 14.4383\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.8317 - val_loss: 12.2369\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.6090 - val_loss: 12.8107\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.5346 - val_loss: 12.4276\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.5026 - val_loss: 11.9487\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.5650 - val_loss: 12.6798\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.4356 - val_loss: 12.2316\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.3950 - val_loss: 12.6787\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.3468 - val_loss: 11.9454\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.6066 - val_loss: 12.9440\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.3376 - val_loss: 13.3578\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.5526 - val_loss: 12.6357\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.3994 - val_loss: 13.0153\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.3673 - val_loss: 12.6828\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.4387 - val_loss: 13.3037\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.5096 - val_loss: 12.7352\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.4530 - val_loss: 12.9599\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.5179 - val_loss: 11.8501\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.4520 - val_loss: 13.1438\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.4839 - val_loss: 12.4571\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.4943 - val_loss: 12.9164\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.2827 - val_loss: 14.3112\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.3279 - val_loss: 13.3992\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.3371 - val_loss: 13.0542\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.3366 - val_loss: 13.2206\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.7545 - val_loss: 12.8584\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.3229 - val_loss: 13.5682\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.3293 - val_loss: 13.1605\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.4681 - val_loss: 13.5523\n",
      "Epoch 191/200\n",
      " - 0s - loss: 6.4832 - val_loss: 12.9244\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.7129 - val_loss: 12.2923\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.2705 - val_loss: 13.5008\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.5065 - val_loss: 12.8425\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.1366 - val_loss: 13.0698\n",
      "Epoch 196/200\n",
      " - 0s - loss: 6.3866 - val_loss: 12.2293\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.3295 - val_loss: 13.3599\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.5038 - val_loss: 12.6081\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.4444 - val_loss: 12.7436\n",
      "Epoch 200/200\n",
      " - 0s - loss: 6.2635 - val_loss: 12.4389\n",
      "Train Index:  [   0    1    2 ... 1913 1914 1915] \n",
      "\n",
      "Test Index:  [1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547\n",
      " 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561\n",
      " 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575\n",
      " 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589\n",
      " 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603\n",
      " 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617\n",
      " 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631\n",
      " 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645\n",
      " 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659\n",
      " 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673\n",
      " 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687\n",
      " 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701\n",
      " 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715\n",
      " 1716 1717 1718 1719 1720 1721 1722 1723 1724]\n",
      "Train on 1380 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 24.4365 - val_loss: 8.2244\n",
      "Epoch 2/200\n",
      " - 0s - loss: 9.8200 - val_loss: 10.3343\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.7492 - val_loss: 10.3804\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.5464 - val_loss: 7.3752\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.3188 - val_loss: 9.6687\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.5204 - val_loss: 7.5287\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.2966 - val_loss: 10.7102\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.2873 - val_loss: 7.7311\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.1700 - val_loss: 8.4094\n",
      "Epoch 10/200\n",
      " - 0s - loss: 8.1543 - val_loss: 7.2213\n",
      "Epoch 11/200\n",
      " - 0s - loss: 8.2161 - val_loss: 8.8513\n",
      "Epoch 12/200\n",
      " - 0s - loss: 8.0230 - val_loss: 8.9848\n",
      "Epoch 13/200\n",
      " - 0s - loss: 8.0744 - val_loss: 9.6047\n",
      "Epoch 14/200\n",
      " - 0s - loss: 8.0819 - val_loss: 9.9381\n",
      "Epoch 15/200\n",
      " - 0s - loss: 8.0309 - val_loss: 9.5660\n",
      "Epoch 16/200\n",
      " - 0s - loss: 7.9546 - val_loss: 6.3052\n",
      "Epoch 17/200\n",
      " - 0s - loss: 8.0335 - val_loss: 10.3444\n",
      "Epoch 18/200\n",
      " - 0s - loss: 7.9430 - val_loss: 7.4887\n",
      "Epoch 19/200\n",
      " - 0s - loss: 8.0023 - val_loss: 6.6679\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.9572 - val_loss: 7.8889\n",
      "Epoch 21/200\n",
      " - 0s - loss: 8.0403 - val_loss: 10.3861\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.9300 - val_loss: 8.0775\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.7512 - val_loss: 8.2831\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.9941 - val_loss: 7.1300\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.7492 - val_loss: 8.5505\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.8814 - val_loss: 8.2859\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.8343 - val_loss: 7.1099\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.8161 - val_loss: 7.5684\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.7972 - val_loss: 8.1515\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.7051 - val_loss: 9.4990\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.8618 - val_loss: 7.1946\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.7776 - val_loss: 6.7952\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.8586 - val_loss: 7.7975\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.8072 - val_loss: 9.5021\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.6990 - val_loss: 7.7357\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.6901 - val_loss: 9.5443\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.6878 - val_loss: 8.8532\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.7557 - val_loss: 9.6262\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.7964 - val_loss: 7.8453\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.5228 - val_loss: 7.5197\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.5321 - val_loss: 7.2695\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.5936 - val_loss: 6.1802\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.5834 - val_loss: 8.1275\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.6177 - val_loss: 7.8794\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.4853 - val_loss: 7.0228\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.5931 - val_loss: 10.2772\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.5447 - val_loss: 9.1080\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.5005 - val_loss: 7.3802\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.5639 - val_loss: 10.3031\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 7.6195 - val_loss: 8.0331\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.4637 - val_loss: 8.4185\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.5112 - val_loss: 8.4490\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.4456 - val_loss: 9.4895\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.6041 - val_loss: 7.8985\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.6044 - val_loss: 8.1206\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.6683 - val_loss: 6.4091\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.3739 - val_loss: 8.8015\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.3810 - val_loss: 9.1128\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.5384 - val_loss: 8.0082\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.7043 - val_loss: 8.6427\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.3183 - val_loss: 10.8027\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.4526 - val_loss: 6.8266\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.2911 - val_loss: 8.7600\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.3617 - val_loss: 7.5056\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.4316 - val_loss: 9.6108\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.3650 - val_loss: 6.5471\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.3677 - val_loss: 7.8122\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.4942 - val_loss: 10.8394\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.1918 - val_loss: 7.1652\n",
      "Epoch 70/200\n",
      " - 0s - loss: 7.2300 - val_loss: 7.2475\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.3237 - val_loss: 7.1794\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.3034 - val_loss: 8.1230\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.3324 - val_loss: 7.2862\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.3795 - val_loss: 9.1432\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.3212 - val_loss: 7.0863\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.1894 - val_loss: 7.5388\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.2354 - val_loss: 6.4902\n",
      "Epoch 78/200\n",
      " - 0s - loss: 7.0787 - val_loss: 9.4616\n",
      "Epoch 79/200\n",
      " - 0s - loss: 7.3037 - val_loss: 8.5613\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.2396 - val_loss: 6.8841\n",
      "Epoch 81/200\n",
      " - 0s - loss: 7.1624 - val_loss: 6.2539\n",
      "Epoch 82/200\n",
      " - 0s - loss: 7.3212 - val_loss: 7.2169\n",
      "Epoch 83/200\n",
      " - 0s - loss: 7.1017 - val_loss: 7.8839\n",
      "Epoch 84/200\n",
      " - 0s - loss: 7.0955 - val_loss: 7.8374\n",
      "Epoch 85/200\n",
      " - 0s - loss: 7.0519 - val_loss: 8.0540\n",
      "Epoch 86/200\n",
      " - 0s - loss: 7.1710 - val_loss: 7.9926\n",
      "Epoch 87/200\n",
      " - 0s - loss: 7.1130 - val_loss: 8.0734\n",
      "Epoch 88/200\n",
      " - 0s - loss: 7.0970 - val_loss: 7.4551\n",
      "Epoch 89/200\n",
      " - 0s - loss: 7.0412 - val_loss: 8.1269\n",
      "Epoch 90/200\n",
      " - 0s - loss: 7.1298 - val_loss: 8.5144\n",
      "Epoch 91/200\n",
      " - 0s - loss: 7.0909 - val_loss: 7.9554\n",
      "Epoch 92/200\n",
      " - 0s - loss: 7.1568 - val_loss: 7.6384\n",
      "Epoch 93/200\n",
      " - 0s - loss: 7.0607 - val_loss: 10.0268\n",
      "Epoch 94/200\n",
      " - 0s - loss: 7.3320 - val_loss: 9.7325\n",
      "Epoch 95/200\n",
      " - 0s - loss: 7.0765 - val_loss: 6.5508\n",
      "Epoch 96/200\n",
      " - 0s - loss: 7.0232 - val_loss: 7.4277\n",
      "Epoch 97/200\n",
      " - 0s - loss: 7.0601 - val_loss: 9.6537\n",
      "Epoch 98/200\n",
      " - 0s - loss: 7.0963 - val_loss: 7.1140\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.9368 - val_loss: 8.3016\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.9239 - val_loss: 9.2529\n",
      "Epoch 101/200\n",
      " - 0s - loss: 6.8663 - val_loss: 6.8916\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.9984 - val_loss: 6.2861\n",
      "Epoch 103/200\n",
      " - 0s - loss: 7.0528 - val_loss: 6.8348\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.9591 - val_loss: 7.7549\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.8545 - val_loss: 7.0756\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.9070 - val_loss: 6.6217\n",
      "Epoch 107/200\n",
      " - 0s - loss: 7.0897 - val_loss: 7.9700\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.8783 - val_loss: 8.2693\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.8359 - val_loss: 8.2843\n",
      "Epoch 110/200\n",
      " - 0s - loss: 6.8809 - val_loss: 8.5299\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.8869 - val_loss: 8.0049\n",
      "Epoch 112/200\n",
      " - 0s - loss: 7.1182 - val_loss: 6.5604\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.8487 - val_loss: 8.8532\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.8406 - val_loss: 6.7192\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.7796 - val_loss: 7.8882\n",
      "Epoch 116/200\n",
      " - 0s - loss: 7.0223 - val_loss: 7.7197\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.8842 - val_loss: 7.0731\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.7927 - val_loss: 7.1074\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.9022 - val_loss: 7.2144\n",
      "Epoch 120/200\n",
      " - 0s - loss: 7.2444 - val_loss: 8.0142\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.7708 - val_loss: 9.9665\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.8415 - val_loss: 10.4122\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.8936 - val_loss: 7.7292\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.8049 - val_loss: 7.3256\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.8821 - val_loss: 9.3244\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.7621 - val_loss: 7.3460\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.6721 - val_loss: 10.0310\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.9301 - val_loss: 7.6450\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.8164 - val_loss: 7.0064\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.8393 - val_loss: 8.6954\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.8293 - val_loss: 6.8381\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.8329 - val_loss: 6.2667\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.7225 - val_loss: 9.9382\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.8211 - val_loss: 8.4120\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.6334 - val_loss: 9.8276\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.6579 - val_loss: 9.2238\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.7171 - val_loss: 9.2969\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.9010 - val_loss: 9.1855\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.6296 - val_loss: 7.2971\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.7473 - val_loss: 7.3725\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.7516 - val_loss: 7.1508\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.7811 - val_loss: 7.9936\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.5465 - val_loss: 9.2730\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.6565 - val_loss: 8.3467\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.7903 - val_loss: 10.3000\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.7098 - val_loss: 7.6174\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.6207 - val_loss: 8.6040\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.5240 - val_loss: 6.9859\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.5957 - val_loss: 8.4926\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.5783 - val_loss: 7.9468\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.7958 - val_loss: 9.2704\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.8968 - val_loss: 6.6630\n",
      "Epoch 153/200\n",
      " - 0s - loss: 6.6285 - val_loss: 10.2799\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.5857 - val_loss: 10.0699\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.6084 - val_loss: 7.2659\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.5884 - val_loss: 7.1895\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.6081 - val_loss: 8.5485\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.4374 - val_loss: 7.1580\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.6018 - val_loss: 8.4270\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.6070 - val_loss: 8.6566\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.5964 - val_loss: 7.1598\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.5079 - val_loss: 9.6112\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.6157 - val_loss: 7.6301\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.4741 - val_loss: 10.1114\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.5614 - val_loss: 8.1940\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.5693 - val_loss: 6.3374\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.4593 - val_loss: 8.8873\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.5945 - val_loss: 8.4240\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.6749 - val_loss: 7.3542\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.7505 - val_loss: 7.8330\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.4225 - val_loss: 7.9163\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.4273 - val_loss: 7.3539\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.5138 - val_loss: 8.2332\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.5135 - val_loss: 7.9384\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.4011 - val_loss: 7.5883\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.2417 - val_loss: 8.0874\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.4120 - val_loss: 9.2500\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.3950 - val_loss: 7.9421\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.4112 - val_loss: 8.4444\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.3765 - val_loss: 9.4251\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.4479 - val_loss: 7.3602\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.5594 - val_loss: 7.5411\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.3634 - val_loss: 9.1758\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.2462 - val_loss: 10.2688\n",
      "Epoch 185/200\n",
      " - 0s - loss: 6.2835 - val_loss: 7.7451\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.3392 - val_loss: 12.4987\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.3756 - val_loss: 10.3313\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.2703 - val_loss: 8.9504\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.0798 - val_loss: 8.5609\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.4464 - val_loss: 8.1327\n",
      "Epoch 191/200\n",
      " - 0s - loss: 6.4000 - val_loss: 9.0928\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.0336 - val_loss: 7.5506\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.3149 - val_loss: 8.6009\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.2903 - val_loss: 8.6022\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.3360 - val_loss: 7.4395\n",
      "Epoch 196/200\n",
      " - 0s - loss: 6.4155 - val_loss: 8.7311\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.3112 - val_loss: 8.2506\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.2712 - val_loss: 9.3537\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.2984 - val_loss: 8.2261\n",
      "Epoch 200/200\n",
      " - 0s - loss: 6.2547 - val_loss: 7.8226\n",
      "Train Index:  [   0    1    2 ... 1722 1723 1724] \n",
      "\n",
      "Test Index:  [1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738\n",
      " 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752\n",
      " 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766\n",
      " 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780\n",
      " 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794\n",
      " 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808\n",
      " 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822\n",
      " 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836\n",
      " 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850\n",
      " 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864\n",
      " 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878\n",
      " 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892\n",
      " 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906\n",
      " 1907 1908 1909 1910 1911 1912 1913 1914 1915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1380 samples, validate on 345 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 24.5076 - val_loss: 13.8023\n",
      "Epoch 2/200\n",
      " - 0s - loss: 9.5653 - val_loss: 10.5387\n",
      "Epoch 3/200\n",
      " - 0s - loss: 8.5385 - val_loss: 9.8332\n",
      "Epoch 4/200\n",
      " - 0s - loss: 8.2928 - val_loss: 10.4113\n",
      "Epoch 5/200\n",
      " - 0s - loss: 8.3562 - val_loss: 10.1665\n",
      "Epoch 6/200\n",
      " - 0s - loss: 8.3900 - val_loss: 9.9148\n",
      "Epoch 7/200\n",
      " - 0s - loss: 8.2902 - val_loss: 9.5354\n",
      "Epoch 8/200\n",
      " - 0s - loss: 8.2431 - val_loss: 9.5408\n",
      "Epoch 9/200\n",
      " - 0s - loss: 8.1669 - val_loss: 10.1232\n",
      "Epoch 10/200\n",
      " - 0s - loss: 8.2161 - val_loss: 10.5880\n",
      "Epoch 11/200\n",
      " - 0s - loss: 8.1036 - val_loss: 9.5178\n",
      "Epoch 12/200\n",
      " - 0s - loss: 8.0332 - val_loss: 9.6987\n",
      "Epoch 13/200\n",
      " - 0s - loss: 7.9812 - val_loss: 10.6616\n",
      "Epoch 14/200\n",
      " - 0s - loss: 8.1028 - val_loss: 10.7151\n",
      "Epoch 15/200\n",
      " - 0s - loss: 8.0070 - val_loss: 9.5851\n",
      "Epoch 16/200\n",
      " - 0s - loss: 8.0069 - val_loss: 9.5183\n",
      "Epoch 17/200\n",
      " - 0s - loss: 7.8806 - val_loss: 9.5757\n",
      "Epoch 18/200\n",
      " - 0s - loss: 8.0484 - val_loss: 10.2338\n",
      "Epoch 19/200\n",
      " - 0s - loss: 7.8886 - val_loss: 10.0011\n",
      "Epoch 20/200\n",
      " - 0s - loss: 7.9445 - val_loss: 9.5234\n",
      "Epoch 21/200\n",
      " - 0s - loss: 8.0252 - val_loss: 10.5920\n",
      "Epoch 22/200\n",
      " - 0s - loss: 7.9080 - val_loss: 9.3340\n",
      "Epoch 23/200\n",
      " - 0s - loss: 7.8573 - val_loss: 10.1264\n",
      "Epoch 24/200\n",
      " - 0s - loss: 7.9026 - val_loss: 9.4853\n",
      "Epoch 25/200\n",
      " - 0s - loss: 7.8521 - val_loss: 11.6471\n",
      "Epoch 26/200\n",
      " - 0s - loss: 7.7789 - val_loss: 10.0206\n",
      "Epoch 27/200\n",
      " - 0s - loss: 7.8545 - val_loss: 10.1949\n",
      "Epoch 28/200\n",
      " - 0s - loss: 7.8148 - val_loss: 9.6227\n",
      "Epoch 29/200\n",
      " - 0s - loss: 7.8295 - val_loss: 10.3259\n",
      "Epoch 30/200\n",
      " - 0s - loss: 7.7618 - val_loss: 10.2934\n",
      "Epoch 31/200\n",
      " - 0s - loss: 7.7592 - val_loss: 9.9978\n",
      "Epoch 32/200\n",
      " - 0s - loss: 7.7129 - val_loss: 10.0603\n",
      "Epoch 33/200\n",
      " - 0s - loss: 7.7357 - val_loss: 11.0185\n",
      "Epoch 34/200\n",
      " - 0s - loss: 7.7378 - val_loss: 10.1340\n",
      "Epoch 35/200\n",
      " - 0s - loss: 7.5657 - val_loss: 9.8553\n",
      "Epoch 36/200\n",
      " - 0s - loss: 7.6531 - val_loss: 10.6064\n",
      "Epoch 37/200\n",
      " - 0s - loss: 7.6303 - val_loss: 9.3723\n",
      "Epoch 38/200\n",
      " - 0s - loss: 7.5304 - val_loss: 9.6927\n",
      "Epoch 39/200\n",
      " - 0s - loss: 7.4940 - val_loss: 10.1802\n",
      "Epoch 40/200\n",
      " - 0s - loss: 7.6593 - val_loss: 9.2793\n",
      "Epoch 41/200\n",
      " - 0s - loss: 7.6892 - val_loss: 9.5998\n",
      "Epoch 42/200\n",
      " - 0s - loss: 7.5927 - val_loss: 9.4313\n",
      "Epoch 43/200\n",
      " - 0s - loss: 7.5142 - val_loss: 10.5010\n",
      "Epoch 44/200\n",
      " - 0s - loss: 7.5189 - val_loss: 9.6829\n",
      "Epoch 45/200\n",
      " - 0s - loss: 7.6241 - val_loss: 9.6996\n",
      "Epoch 46/200\n",
      " - 0s - loss: 7.5048 - val_loss: 9.6495\n",
      "Epoch 47/200\n",
      " - 0s - loss: 7.3612 - val_loss: 9.0920\n",
      "Epoch 48/200\n",
      " - 0s - loss: 7.6386 - val_loss: 9.3733\n",
      "Epoch 49/200\n",
      " - 0s - loss: 7.6323 - val_loss: 10.1038\n",
      "Epoch 50/200\n",
      " - 0s - loss: 7.5052 - val_loss: 9.5039\n",
      "Epoch 51/200\n",
      " - 0s - loss: 7.3689 - val_loss: 9.7539\n",
      "Epoch 52/200\n",
      " - 0s - loss: 7.4459 - val_loss: 10.2880\n",
      "Epoch 53/200\n",
      " - 0s - loss: 7.4159 - val_loss: 9.5259\n",
      "Epoch 54/200\n",
      " - 0s - loss: 7.4061 - val_loss: 9.4119\n",
      "Epoch 55/200\n",
      " - 0s - loss: 7.3721 - val_loss: 10.0855\n",
      "Epoch 56/200\n",
      " - 0s - loss: 7.3770 - val_loss: 8.9217\n",
      "Epoch 57/200\n",
      " - 0s - loss: 7.3400 - val_loss: 9.7086\n",
      "Epoch 58/200\n",
      " - 0s - loss: 7.3207 - val_loss: 9.4688\n",
      "Epoch 59/200\n",
      " - 0s - loss: 7.3671 - val_loss: 10.7196\n",
      "Epoch 60/200\n",
      " - 0s - loss: 7.3290 - val_loss: 9.4222\n",
      "Epoch 61/200\n",
      " - 0s - loss: 7.4104 - val_loss: 9.6556\n",
      "Epoch 62/200\n",
      " - 0s - loss: 7.2583 - val_loss: 9.2978\n",
      "Epoch 63/200\n",
      " - 0s - loss: 7.1980 - val_loss: 9.6159\n",
      "Epoch 64/200\n",
      " - 0s - loss: 7.2522 - val_loss: 9.4678\n",
      "Epoch 65/200\n",
      " - 0s - loss: 7.2488 - val_loss: 9.3442\n",
      "Epoch 66/200\n",
      " - 0s - loss: 7.3080 - val_loss: 9.6649\n",
      "Epoch 67/200\n",
      " - 0s - loss: 7.4417 - val_loss: 9.9031\n",
      "Epoch 68/200\n",
      " - 0s - loss: 7.0635 - val_loss: 9.0600\n",
      "Epoch 69/200\n",
      " - 0s - loss: 7.2356 - val_loss: 10.1428\n",
      "Epoch 70/200\n",
      " - 0s - loss: 7.1714 - val_loss: 9.8721\n",
      "Epoch 71/200\n",
      " - 0s - loss: 7.1860 - val_loss: 9.3065\n",
      "Epoch 72/200\n",
      " - 0s - loss: 7.0933 - val_loss: 9.8027\n",
      "Epoch 73/200\n",
      " - 0s - loss: 7.2361 - val_loss: 9.5259\n",
      "Epoch 74/200\n",
      " - 0s - loss: 7.1554 - val_loss: 10.0365\n",
      "Epoch 75/200\n",
      " - 0s - loss: 7.1718 - val_loss: 9.9060\n",
      "Epoch 76/200\n",
      " - 0s - loss: 7.0331 - val_loss: 9.7557\n",
      "Epoch 77/200\n",
      " - 0s - loss: 7.0073 - val_loss: 9.8196\n",
      "Epoch 78/200\n",
      " - 0s - loss: 7.0373 - val_loss: 10.0012\n",
      "Epoch 79/200\n",
      " - 0s - loss: 6.8721 - val_loss: 9.7681\n",
      "Epoch 80/200\n",
      " - 0s - loss: 7.0562 - val_loss: 9.6394\n",
      "Epoch 81/200\n",
      " - 0s - loss: 7.0524 - val_loss: 10.6037\n",
      "Epoch 82/200\n",
      " - 0s - loss: 7.1725 - val_loss: 9.3679\n",
      "Epoch 83/200\n",
      " - 0s - loss: 7.0645 - val_loss: 9.0705\n",
      "Epoch 84/200\n",
      " - 0s - loss: 7.0546 - val_loss: 10.0101\n",
      "Epoch 85/200\n",
      " - 0s - loss: 7.0121 - val_loss: 9.0080\n",
      "Epoch 86/200\n",
      " - 0s - loss: 6.9589 - val_loss: 9.4990\n",
      "Epoch 87/200\n",
      " - 0s - loss: 6.9413 - val_loss: 9.6488\n",
      "Epoch 88/200\n",
      " - 0s - loss: 6.9554 - val_loss: 10.3197\n",
      "Epoch 89/200\n",
      " - 0s - loss: 6.9637 - val_loss: 9.8201\n",
      "Epoch 90/200\n",
      " - 0s - loss: 6.9725 - val_loss: 9.8043\n",
      "Epoch 91/200\n",
      " - 0s - loss: 6.9956 - val_loss: 9.6209\n",
      "Epoch 92/200\n",
      " - 0s - loss: 6.9104 - val_loss: 9.3701\n",
      "Epoch 93/200\n",
      " - 0s - loss: 6.8997 - val_loss: 9.8920\n",
      "Epoch 94/200\n",
      " - 0s - loss: 6.9294 - val_loss: 9.2964\n",
      "Epoch 95/200\n",
      " - 0s - loss: 6.9142 - val_loss: 9.8360\n",
      "Epoch 96/200\n",
      " - 0s - loss: 7.0743 - val_loss: 10.0749\n",
      "Epoch 97/200\n",
      " - 0s - loss: 6.8923 - val_loss: 9.1495\n",
      "Epoch 98/200\n",
      " - 0s - loss: 6.7735 - val_loss: 10.0226\n",
      "Epoch 99/200\n",
      " - 0s - loss: 6.9440 - val_loss: 9.0513\n",
      "Epoch 100/200\n",
      " - 0s - loss: 6.9229 - val_loss: 9.6128\n",
      "Epoch 101/200\n",
      " - 0s - loss: 7.1319 - val_loss: 9.4428\n",
      "Epoch 102/200\n",
      " - 0s - loss: 6.8940 - val_loss: 10.0366\n",
      "Epoch 103/200\n",
      " - 0s - loss: 6.7075 - val_loss: 9.3947\n",
      "Epoch 104/200\n",
      " - 0s - loss: 6.7071 - val_loss: 9.2534\n",
      "Epoch 105/200\n",
      " - 0s - loss: 6.5969 - val_loss: 9.6183\n",
      "Epoch 106/200\n",
      " - 0s - loss: 6.7319 - val_loss: 9.2897\n",
      "Epoch 107/200\n",
      " - 0s - loss: 6.7382 - val_loss: 9.3412\n",
      "Epoch 108/200\n",
      " - 0s - loss: 6.6772 - val_loss: 10.5688\n",
      "Epoch 109/200\n",
      " - 0s - loss: 6.7077 - val_loss: 9.5980\n",
      "Epoch 110/200\n",
      " - 0s - loss: 7.1960 - val_loss: 9.5311\n",
      "Epoch 111/200\n",
      " - 0s - loss: 6.7596 - val_loss: 9.3354\n",
      "Epoch 112/200\n",
      " - 0s - loss: 6.7475 - val_loss: 9.6222\n",
      "Epoch 113/200\n",
      " - 0s - loss: 6.6581 - val_loss: 10.2391\n",
      "Epoch 114/200\n",
      " - 0s - loss: 6.6983 - val_loss: 9.2038\n",
      "Epoch 115/200\n",
      " - 0s - loss: 6.5698 - val_loss: 9.1887\n",
      "Epoch 116/200\n",
      " - 0s - loss: 6.6679 - val_loss: 9.6577\n",
      "Epoch 117/200\n",
      " - 0s - loss: 6.6817 - val_loss: 10.4839\n",
      "Epoch 118/200\n",
      " - 0s - loss: 6.7153 - val_loss: 9.2851\n",
      "Epoch 119/200\n",
      " - 0s - loss: 6.6665 - val_loss: 9.2800\n",
      "Epoch 120/200\n",
      " - 0s - loss: 6.6964 - val_loss: 9.3700\n",
      "Epoch 121/200\n",
      " - 0s - loss: 6.5807 - val_loss: 8.8804\n",
      "Epoch 122/200\n",
      " - 0s - loss: 6.6822 - val_loss: 9.9053\n",
      "Epoch 123/200\n",
      " - 0s - loss: 6.6451 - val_loss: 11.0127\n",
      "Epoch 124/200\n",
      " - 0s - loss: 6.6147 - val_loss: 10.1072\n",
      "Epoch 125/200\n",
      " - 0s - loss: 6.4731 - val_loss: 9.5906\n",
      "Epoch 126/200\n",
      " - 0s - loss: 6.4757 - val_loss: 9.7539\n",
      "Epoch 127/200\n",
      " - 0s - loss: 6.3979 - val_loss: 9.2512\n",
      "Epoch 128/200\n",
      " - 0s - loss: 6.5532 - val_loss: 9.6993\n",
      "Epoch 129/200\n",
      " - 0s - loss: 6.4976 - val_loss: 9.5304\n",
      "Epoch 130/200\n",
      " - 0s - loss: 6.4545 - val_loss: 9.7577\n",
      "Epoch 131/200\n",
      " - 0s - loss: 6.7066 - val_loss: 9.0447\n",
      "Epoch 132/200\n",
      " - 0s - loss: 6.6935 - val_loss: 9.7981\n",
      "Epoch 133/200\n",
      " - 0s - loss: 6.4216 - val_loss: 9.0712\n",
      "Epoch 134/200\n",
      " - 0s - loss: 6.4857 - val_loss: 9.9829\n",
      "Epoch 135/200\n",
      " - 0s - loss: 6.5080 - val_loss: 9.1295\n",
      "Epoch 136/200\n",
      " - 0s - loss: 6.5455 - val_loss: 9.9446\n",
      "Epoch 137/200\n",
      " - 0s - loss: 6.6385 - val_loss: 9.3687\n",
      "Epoch 138/200\n",
      " - 0s - loss: 6.5740 - val_loss: 10.2948\n",
      "Epoch 139/200\n",
      " - 0s - loss: 6.5017 - val_loss: 10.1993\n",
      "Epoch 140/200\n",
      " - 0s - loss: 6.5047 - val_loss: 9.3892\n",
      "Epoch 141/200\n",
      " - 0s - loss: 6.4578 - val_loss: 9.1350\n",
      "Epoch 142/200\n",
      " - 0s - loss: 6.4012 - val_loss: 10.0890\n",
      "Epoch 143/200\n",
      " - 0s - loss: 6.4857 - val_loss: 10.7695\n",
      "Epoch 144/200\n",
      " - 0s - loss: 6.4484 - val_loss: 9.7127\n",
      "Epoch 145/200\n",
      " - 0s - loss: 6.3832 - val_loss: 9.7953\n",
      "Epoch 146/200\n",
      " - 0s - loss: 6.5733 - val_loss: 10.0480\n",
      "Epoch 147/200\n",
      " - 0s - loss: 6.4520 - val_loss: 9.6053\n",
      "Epoch 148/200\n",
      " - 0s - loss: 6.3417 - val_loss: 9.6787\n",
      "Epoch 149/200\n",
      " - 0s - loss: 6.2152 - val_loss: 9.3375\n",
      "Epoch 150/200\n",
      " - 0s - loss: 6.2058 - val_loss: 9.5896\n",
      "Epoch 151/200\n",
      " - 0s - loss: 6.3752 - val_loss: 9.4370\n",
      "Epoch 152/200\n",
      " - 0s - loss: 6.3415 - val_loss: 9.5694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200\n",
      " - 0s - loss: 6.3534 - val_loss: 10.8265\n",
      "Epoch 154/200\n",
      " - 0s - loss: 6.4543 - val_loss: 9.2103\n",
      "Epoch 155/200\n",
      " - 0s - loss: 6.4973 - val_loss: 9.1838\n",
      "Epoch 156/200\n",
      " - 0s - loss: 6.5516 - val_loss: 10.4490\n",
      "Epoch 157/200\n",
      " - 0s - loss: 6.3837 - val_loss: 10.1199\n",
      "Epoch 158/200\n",
      " - 0s - loss: 6.5758 - val_loss: 9.4092\n",
      "Epoch 159/200\n",
      " - 0s - loss: 6.4199 - val_loss: 10.7168\n",
      "Epoch 160/200\n",
      " - 0s - loss: 6.2527 - val_loss: 9.4498\n",
      "Epoch 161/200\n",
      " - 0s - loss: 6.5127 - val_loss: 9.3546\n",
      "Epoch 162/200\n",
      " - 0s - loss: 6.2760 - val_loss: 9.7350\n",
      "Epoch 163/200\n",
      " - 0s - loss: 6.3186 - val_loss: 10.1510\n",
      "Epoch 164/200\n",
      " - 0s - loss: 6.4047 - val_loss: 9.5992\n",
      "Epoch 165/200\n",
      " - 0s - loss: 6.3414 - val_loss: 9.5199\n",
      "Epoch 166/200\n",
      " - 0s - loss: 6.3383 - val_loss: 9.5957\n",
      "Epoch 167/200\n",
      " - 0s - loss: 6.1878 - val_loss: 9.8225\n",
      "Epoch 168/200\n",
      " - 0s - loss: 6.1907 - val_loss: 9.3472\n",
      "Epoch 169/200\n",
      " - 0s - loss: 6.2052 - val_loss: 11.2252\n",
      "Epoch 170/200\n",
      " - 0s - loss: 6.1537 - val_loss: 9.4704\n",
      "Epoch 171/200\n",
      " - 0s - loss: 6.2728 - val_loss: 9.7485\n",
      "Epoch 172/200\n",
      " - 0s - loss: 6.1632 - val_loss: 10.0076\n",
      "Epoch 173/200\n",
      " - 0s - loss: 6.1672 - val_loss: 9.6287\n",
      "Epoch 174/200\n",
      " - 0s - loss: 6.2514 - val_loss: 9.9170\n",
      "Epoch 175/200\n",
      " - 0s - loss: 6.3037 - val_loss: 9.8258\n",
      "Epoch 176/200\n",
      " - 0s - loss: 6.4908 - val_loss: 9.2195\n",
      "Epoch 177/200\n",
      " - 0s - loss: 6.1218 - val_loss: 10.3105\n",
      "Epoch 178/200\n",
      " - 0s - loss: 6.1741 - val_loss: 9.5582\n",
      "Epoch 179/200\n",
      " - 0s - loss: 6.0709 - val_loss: 9.6411\n",
      "Epoch 180/200\n",
      " - 0s - loss: 6.1385 - val_loss: 9.7746\n",
      "Epoch 181/200\n",
      " - 0s - loss: 6.0829 - val_loss: 9.4678\n",
      "Epoch 182/200\n",
      " - 0s - loss: 6.1038 - val_loss: 9.9071\n",
      "Epoch 183/200\n",
      " - 0s - loss: 6.1973 - val_loss: 9.9184\n",
      "Epoch 184/200\n",
      " - 0s - loss: 6.1260 - val_loss: 9.5573\n",
      "Epoch 185/200\n",
      " - 0s - loss: 5.9200 - val_loss: 9.8803\n",
      "Epoch 186/200\n",
      " - 0s - loss: 6.5408 - val_loss: 9.7252\n",
      "Epoch 187/200\n",
      " - 0s - loss: 6.2095 - val_loss: 8.8063\n",
      "Epoch 188/200\n",
      " - 0s - loss: 6.1909 - val_loss: 16.5187\n",
      "Epoch 189/200\n",
      " - 0s - loss: 6.6500 - val_loss: 13.2427\n",
      "Epoch 190/200\n",
      " - 0s - loss: 6.0114 - val_loss: 10.7646\n",
      "Epoch 191/200\n",
      " - 0s - loss: 5.9765 - val_loss: 9.7531\n",
      "Epoch 192/200\n",
      " - 0s - loss: 6.0198 - val_loss: 9.9602\n",
      "Epoch 193/200\n",
      " - 0s - loss: 6.0114 - val_loss: 10.0527\n",
      "Epoch 194/200\n",
      " - 0s - loss: 6.1433 - val_loss: 11.0221\n",
      "Epoch 195/200\n",
      " - 0s - loss: 6.0290 - val_loss: 10.0776\n",
      "Epoch 196/200\n",
      " - 0s - loss: 5.9866 - val_loss: 10.5230\n",
      "Epoch 197/200\n",
      " - 0s - loss: 6.0053 - val_loss: 9.9573\n",
      "Epoch 198/200\n",
      " - 0s - loss: 6.0710 - val_loss: 9.6180\n",
      "Epoch 199/200\n",
      " - 0s - loss: 6.0164 - val_loss: 9.2813\n",
      "Epoch 200/200\n",
      " - 0s - loss: 5.8954 - val_loss: 10.1271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "import keras \n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "\n",
    "scores = []\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(\"Train Index: \", train_index, \"\\n\")\n",
    "    print(\"Test Index: \", test_index)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=7,activation='sigmoid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='relu'))\n",
    "    # Compile model\n",
    "    X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split = 0.2, verbose=2)\n",
    "    ron =model.predict(X_test)\n",
    "    lol = pd.DataFrame(ron)\n",
    "    lol = lol.rename(columns={0:'a',1:'b'})\n",
    "    actual_1=pd.DataFrame(np.array(y_test).T)\n",
    "    actual = actual_1.T\n",
    "    actual = actual.rename(columns={0:'x',1:'y'})\n",
    "    actual['a']=lol['a']\n",
    "    actual['b'] = lol['b']\n",
    "    actual['distance']=np.sqrt((actual['x']-actual['a'])**2+(actual['y']-actual['b'])**2)\n",
    "    mean = actual['distance'].mean()\n",
    "    scores.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.056883057895618"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.82137858325047,\n",
       " 2.9699783228288084,\n",
       " 3.4046754706321707,\n",
       " 5.346286430601702,\n",
       " 4.024002739993342,\n",
       " 3.5006812787530968,\n",
       " 4.203743896732098,\n",
       " 2.8731456360155314,\n",
       " 4.579504725058584,\n",
       " 4.056883057895618]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'MLP')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAC39JREFUeJzt3VGopHd5x/Hf010xUmqM5rQN2di9sPRG1LYHK3ijqS1plXih0hRsTVFCoaJQimApagKlCIXaIlTSKk0t1UiosAYtKhqkUCNnMSaKXoSimCrkpLum2KIQfXqxE7o5nt0zc3Z2J/vs5wPDvjPvf+Y8C7vf8/LfmbPV3QFglp/a9AAArJ+4Awwk7gADiTvAQOIOMJC4Awwk7gADiTvAQOIOMNDRTX3ha6+9to8fP76pLw9wWTp58uRj3b110LqNxf348ePZ2dnZ1JcHuCxV1beWWWdbBmAgcQcYSNwBBhJ3gIHEHWAgcQcYSNwBBhJ3gIE29iEmuFSq6pJ8Hf8fMU8n4s54q0a3qoSay55tGYCBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1goKXiXlXfrKqHquqBqtrZ53xV1d9U1cNV9WBV/cr6RwVgWUdXWPvK7n7sHOd+K8kvLm6/luRvF78CsAHr2pZ5bZJ/7DO+mOQ5VXXdml4bgBUtG/dO8umqOllVt+1z/vok3z7r/iOLxwDYgGW3ZV7e3d+pqp9N8pmq+kZ3f+Gs87XPc3rvA4tvDLclyfOf//yVhwVgOUtduXf3dxa/Pprk40leumfJI0luOOv+sSTf2ed17uzu7e7e3traOtzEABzowLhX1U9X1c88eZzkN5N8dc+yE0l+f/GumZcleby7v7v2aQFYyjLbMj+X5ONV9eT6f+7uf62qP0yS7v5Akk8m+e0kDyf53yR/cHHGBWAZB8a9u/8jyYv3efwDZx13kj9a72gAHJZPqAIMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEDiDjCQuAMMJO4AA4k7wEBLx72qjlTVl6vq3n3O3VpVu1X1wOL2lvWOCcAqjq6w9u1Jvp7k2ec4f3d3v/XCRwLgQi115V5Vx5K8OsnfX9xxAFiHZbdl3pfkHUl+fJ41r6uqB6vqnqq64cJHA+CwDox7Vb0myaPdffI8yz6R5Hh3vyjJZ5PcdY7Xuq2qdqpqZ3d391ADA3Cw6u7zL6j6iyS/l+SJJFflzJ77v3T3G8+x/kiSU9199fled3t7u3d2dg41NFeu5z73uTl9+vSmx1iLa665JqdOndr0GFxmqupkd28ftO7Af1Dt7ncmeefiRV+R5E/2hr2qruvu7y7u3pwz//AKa3f69OkcdEFyuaiqTY/AYKu8W+YpquqOJDvdfSLJ26rq5py5uj+V5Nb1jAfAYRy4LXOx2JbhMKpq1JX7lN8Ll86y2zI+oQowkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAOJO8BA4g4wkLgDDCTuAAMtHfeqOlJVX66qe/c598yquruqHq6q+6vq+DqHBGA1R1dY+/YkX0/y7H3OvTnJ6e5+QVXdkuS9SX5nDfPBU/S7n5285+pNj7EW/e79/irBeiwV96o6luTVSf48yR/vs+S1Sd6zOL4nyfurqrq71zEkPKlu/+9M+WNVVen3bHoKplp2W+Z9Sd6R5MfnOH99km8nSXc/keTxJM+74OkAOJQD415Vr0nyaHefPN+yfR77icurqrqtqnaqamd3d3eFMQFYxTJX7i9PcnNVfTPJR5PcWFX/tGfNI0luSJKqOprk6iSn9r5Qd9/Z3dvdvb21tXVBgwNwbgfGvbvf2d3Huvt4kluSfK6737hn2Ykkb1ocv36xZsbGKMBlaJV3yzxFVd2RZKe7TyT5YJIPV9XDOXPFfsua5gPgEFaKe3ffl+S+xfG7znr8B0nesM7BADg8n1AFGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBxB1gIHEHGEjcAQYSd4CBDox7VV1VVV+qqq9U1deq6vZ91txaVbtV9cDi9paLMy4Ayzi6xJofJrmxu79fVc9I8m9V9anu/uKedXd391vXPyIAqzow7t3dSb6/uPuMxa0v5lAAXJil9tyr6khVPZDk0SSf6e7791n2uqp6sKruqaob1jolACtZKu7d/aPufkmSY0leWlUv3LPkE0mOd/eLknw2yV37vU5V3VZVO1W1s7u7eyFzA3AeK71bpru/l+S+JDftefy/uvuHi7t/l+RXz/H8O7t7u7u3t7a2DjEuAMtY5t0yW1X1nMXxs5K8Ksk39qy57qy7Nyf5+jqHBGA1y7xb5rokd1XVkZz5ZvCx7r63qu5IstPdJ5K8rapuTvJEklNJbr1YAwNwsDrzZphLb3t7u3d2djbytbl8VVU29Wd23Sb9Xrh0qupkd28ftM4nVAEGWmZbBp5WqmrTI6zFNddcs+kRGEzcuaxcim0M2yVMYFsGYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEGEneAgQ6Me1VdVVVfqqqvVNXXqur2fdY8s6rurqqHq+r+qjp+MYYFYDnLXLn/MMmN3f3iJC9JclNVvWzPmjcnOd3dL0jyV0neu94xAVjFgXHvM76/uPuMxa33LHttkrsWx/ck+fWqqrVNCcBKltpzr6ojVfVAkkeTfKa779+z5Pok306S7n4iyeNJnrfOQQFY3lJx7+4fdfdLkhxL8tKqeuGeJftdpe+9uk9V3VZVO1W1s7u7u/q0ACxlpXfLdPf3ktyX5KY9px5JckOSVNXRJFcnObXP8+/s7u3u3t7a2jrUwAAcbJl3y2xV1XMWx89K8qok39iz7ESSNy2OX5/kc939E1fuAFwaR5dYc12Su6rqSM58M/hYd99bVXck2enuE0k+mOTDVfVwzlyx33LRJgbgQAfGvbsfTPLL+zz+rrOOf5DkDesdDYDD8glVgIHEHWAgcQcYSNwBBhJ3gIGWeSskXNYO82OODvMcH+3g6UTcGU90uRLZlgEYSNwBBhJ3gIHEHWAgcQcYSNwBBhJ3gIHEHWCg2tQHPKpqN8m3NvLF4fyuTfLYpoeAc/iF7j7w/yndWNzh6aqqdrp7e9NzwIWwLQMwkLgDDCTu8JPu3PQAcKHsuQMM5ModYCBxh4Wq+lBVPVpVX930LHChxB3+3z8kuWnTQ8A6iDssdPcXkpza9BywDuIOMJC4Awwk7gADiTvAQOIOC1X1kST/nuSXquqRqnrzpmeCw/IJVYCBXLkDDCTuAAOJO8BA4g4wkLgDDCTuXFGqqqvqw2fdP1pVu1V17+L+rVX1/n2e982qeqiqvlJVn66qn7+Uc8OqxJ0rzf8keWFVPWtx/zeS/OeSz31ld784yU6SP70Yw8G6iDtXok8lefXi+HeTfGTF538hyQvWOhGsmbhzJfpokluq6qokL0py/4rPf02Sh9Y+FazR0U0PAJdadz9YVcdz5qr9kys89fNV9aMkDyb5s4swGqyNuHOlOpHkL5O8IsnzlnzOK7v7sYs2EayRuHOl+lCSx7v7oap6xaaHgXWz584Vqbsf6e6/PsfpWxc/FfLJ27FLOhysgZ8KCTCQK3eAgcQdYCBxBxhI3AEGEneAgcQdYCBxBxhI3AEG+j/6xnff9nrzPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.boxplot(scores)\n",
    "plt.xlabel('MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
